# Megatron-LM æ¶æ„åˆ†æä¸å…³é”®ç‰¹æ€§

## æ¦‚è¿°

Megatron-LM æ˜¯ NVIDIA å¼€å‘çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œæ—¨åœ¨è·¨æ•°åƒä¸ª GPU è®­ç»ƒæ•°ä¸‡äº¿å‚æ•°çš„æ¨¡å‹ã€‚å®ƒæ˜¯å…ˆè¿›åˆ†å¸ƒå¼è®­ç»ƒæŠ€æœ¯çš„ç”Ÿäº§çº§å®ç°ï¼Œç»“åˆäº†å¼ é‡å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œï¼Œå®ç°äº†å‰æ‰€æœªæœ‰çš„è§„æ¨¡ã€‚

## æ ¸å¿ƒæ¶æ„

### 1. **åŒå±‚è®¾è®¡**

Megatron-LM ç”±ä¸¤ä¸ªä¸»è¦ç»„ä»¶ç»„æˆï¼š

#### **Megatron Core** (`megatron/core/`) ğŸ† MCore
ä¸€ä¸ªç”Ÿäº§çº§çš„åº“ï¼ŒåŒ…å«é¢å‘æ¡†æ¶å¼€å‘è€…çš„ GPU ä¼˜åŒ–æ„å»ºå—ã€‚è¿™ç§æ¨¡å—åŒ–è®¾è®¡å®ç°äº†ï¼š

- å¯ç»„åˆçš„ç»„ä»¶ï¼Œå¯é›†æˆåˆ°å…¶ä»–è®­ç»ƒæ¡†æ¶
- åŸºç¡€è®¾æ–½å’Œè®­ç»ƒé€»è¾‘çš„æ¸…æ™°åˆ†ç¦»
- å¯é‡ç”¨çš„å¹¶è¡ŒåŸè¯­

#### **Training Framework** (`megatron/training/`)
ä½¿ç”¨ Megatron Core è¿›è¡Œç«¯åˆ°ç«¯æ¨¡å‹è®­ç»ƒçš„é«˜çº§è®­ç»ƒè„šæœ¬å’Œå·¥å…·ï¼š

- `training.py` - ä¸»è®­ç»ƒå¾ªç¯å®ç°
- `arguments.py` - å‘½ä»¤è¡Œå‚æ•°è§£æ
- `checkpointing.py` - æ¨¡å‹çŠ¶æ€ç®¡ç†
- `initialize.py` - åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®

### 2. **åˆ†å±‚æ¨¡å‹æ¶æ„ï¼ˆæ ‡æ³¨ MCoreï¼‰**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         Training Framework Layer [megatron/training/]          â•‘
â•‘  training.py | arguments.py | checkpointing.py | initialize.py â•‘
â•‘                     è®­ç»ƒæ¡†æ¶å±‚ (é MCore)                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                              â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              Models Layer ğŸ† MCore [megatron/core/models/]     â•‘
â•‘   GPT | BERT | T5 | Mamba | Multimodal | MoE                   â•‘
â•‘                     æ¨¡å‹å±‚ (MCore)                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                              â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         Transformer Core ğŸ† MCore [megatron/core/transformer/] â•‘
â•‘  Attention | MLP | LayerNorm | Embeddings | Blocks            â•‘
â•‘                   Transformeræ ¸å¿ƒå±‚ (MCore)                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                              â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘     Parallelism Strategy ğŸ† MCore [megatron/core/*/parallel/]  â•‘
â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â•‘
â•‘  â”‚  Tensor     â”‚  Pipeline    â”‚    Data      â”‚  Sequence    â”‚ â•‘
â•‘  â”‚  Parallel   â”‚  Parallel    â”‚  Parallel    â”‚  Parallel    â”‚ â•‘
â•‘  â”‚  [core/tp/] â”‚  [core/pp/]  â”‚  [core/dp/]  â”‚  [core/sp/]  â”‚ â•‘
â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â•‘
â•‘              å¹¶è¡Œç­–ç•¥å±‚ (MCore - å…¨éƒ¨å¹¶è¡Œå®ç°)                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                              â†“
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           Distributed Communication ğŸ† MCore                   â•‘
â•‘     NCCL | Process Groups | P2P Communication                 â•‘
â•‘                 åˆ†å¸ƒå¼é€šä¿¡å±‚ (MCore)                             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**å›¾ä¾‹è¯´æ˜ï¼š**
- ğŸ† MCore = Megatron Core ç»„ä»¶ï¼Œä½äº `megatron/core/` ç›®å½•
- æœªæ ‡æ³¨ = Training Framework ç»„ä»¶ï¼Œä½äº `megatron/training/` ç›®å½•

## MCore ç›®å½•ç»“æ„è¯¦è§£

`megatron/core/` æ˜¯ Megatron-LM çš„æ ¸å¿ƒåº“ï¼ˆMCoreï¼‰ï¼ŒåŒ…å«ç”Ÿäº§çº§çš„ã€GPU ä¼˜åŒ–çš„æ„å»ºå—ï¼š

```
megatron/core/                    ğŸ† MCore æ ¹ç›®å½•
â”‚
â”œâ”€â”€ models/                       ğŸ† æ¨¡å‹å®ç° [MCore]
â”‚   â”œâ”€â”€ gpt/                     â†’ GPT æ¨¡å‹ (gpt_model.py)
â”‚   â”œâ”€â”€ bert/                    â†’ BERT æ¨¡å‹ (bert_model.py)
â”‚   â”œâ”€â”€ t5/                      â†’ T5 æ¨¡å‹ (t5_model.py)
â”‚   â”œâ”€â”€ mamba/                   â†’ Mamba çŠ¶æ€ç©ºé—´æ¨¡å‹
â”‚   â”œâ”€â”€ multimodal/              â†’ å¤šæ¨¡æ€æ¨¡å‹
â”‚   â””â”€â”€ mimo/                    â†’ MoE æ··åˆä¸“å®¶æ¨¡å‹
â”‚
â”œâ”€â”€ transformer/                  ğŸ† Transformeræ„å»ºå— [MCore]
â”‚   â”œâ”€â”€ attention.py             â†’ å¤šå¤´æ³¨æ„åŠ›å®ç°
â”‚   â”œâ”€â”€ mlp.py                   â†’ å‰é¦ˆç½‘ç»œ
â”‚   â”œâ”€â”€ transformer_layer.py     â†’ å•å±‚ Transformer
â”‚   â”œâ”€â”€ transformer_block.py     â†’ Transformer å—
â”‚   â”œâ”€â”€ transformer_config.py    â†’ é…ç½®ç±»
â”‚   â””â”€â”€ fusions/                 â†’ èåˆæ“ä½œ (å­ç›®å½•)
â”‚
â”œâ”€â”€ fusions/                      ğŸ† èåˆæ“ä½œ [MCore]
â”‚   â”œâ”€â”€ fused_bias_dropout.py    â†’ Bias + Dropout èåˆ
â”‚   â”œâ”€â”€ fused_bias_gelu.py       â†’ Bias + GELU èåˆ
â”‚   â”œâ”€â”€ fused_bias_swiglu.py     â†’ Bias + SwiGLU èåˆ
â”‚   â”œâ”€â”€ fused_bias_geglu.py      â†’ Bias + GEGLU èåˆ
â”‚   â”œâ”€â”€ fused_layer_norm.py      â†’ LayerNorm èåˆ
â”‚   â”œâ”€â”€ fused_softmax.py         â†’ Softmax èåˆ
â”‚   â”œâ”€â”€ fused_cross_entropy.py   â†’ äº¤å‰ç†µæŸå¤±èåˆ
â”‚   â””â”€â”€ fused_weighted_squared_relu.py â†’ WSReLU èåˆ
â”‚
â”œâ”€â”€ tensor_parallel/             ğŸ† å¼ é‡å¹¶è¡Œ [MCore]
â”‚   â”œâ”€â”€ layers.py                â†’ å¹¶è¡Œå±‚å®ç°
â”‚   â”œâ”€â”€ mappings.py              â†’ é€šä¿¡æ¨¡å¼ (all-gather, reduce-scatter)
â”‚   â”œâ”€â”€ cross_entropy.py         â†’ å¹¶è¡Œäº¤å‰ç†µæŸå¤±
â”‚   â””â”€â”€ random.py                â†’ å¹¶è¡Œéšæœºæ•°ç”Ÿæˆ
â”‚
â”œâ”€â”€ pipeline_parallel/           ğŸ† æµæ°´çº¿å¹¶è¡Œ [MCore]
â”‚   â”œâ”€â”€ schedules.py             â†’ è°ƒåº¦ç­–ç•¥ (1F1B, GPipe, interleaved)
â”‚   â”œâ”€â”€ p2p_communication.py     â†’ ç‚¹å¯¹ç‚¹é€šä¿¡
â”‚   â””â”€â”€ hybrid_cp_schedule.py    â†’ æ··åˆä¸Šä¸‹æ–‡å¹¶è¡Œè°ƒåº¦
â”‚
â”œâ”€â”€ distributed/                 ğŸ† åˆ†å¸ƒå¼è®­ç»ƒ [MCore]
â”‚   â”œâ”€â”€ distributed_data_parallel.py   â†’ DDP å®ç°
â”‚   â”œâ”€â”€ torch_fully_sharded_data_parallel.py â†’ FSDP å®ç°
â”‚   â””â”€â”€ param_and_grad_buffer.py        â†’ æ¢¯åº¦ç¼“å†²ç®¡ç†
â”‚
â”œâ”€â”€ dist_checkpointing/          ğŸ† åˆ†å¸ƒå¼æ£€æŸ¥ç‚¹ [MCore]
â”‚   â”œâ”€â”€ mapping.py               â†’ åˆ†ç‰‡æ˜ å°„
â”‚   â”œâ”€â”€ core.py                  â†’ æ ¸å¿ƒæ£€æŸ¥ç‚¹åŠŸèƒ½
â”‚   â”œâ”€â”€ optimizer.py             â†’ ä¼˜åŒ–å™¨çŠ¶æ€ä¿å­˜
â”‚   â”œâ”€â”€ serialization.py         â†’ åºåˆ—åŒ–å·¥å…·
â”‚   â””â”€â”€ strategies/              â†’ ä¿å­˜ç­–ç•¥
â”‚
â”œâ”€â”€ optimizer/                   ğŸ† ä¼˜åŒ–å™¨ [MCore]
â”‚   â”œâ”€â”€ distrib_optimizer.py     â†’ åˆ†å¸ƒå¼ä¼˜åŒ–å™¨
â”‚   â”œâ”€â”€ optimizer.py             â†’ åŸºç¡€ä¼˜åŒ–å™¨
â”‚   â””â”€â”€ grad_scaler.py           â†’ æ··åˆç²¾åº¦æ¢¯åº¦ç¼©æ”¾
â”‚
â”œâ”€â”€ datasets/                    ğŸ† æ•°æ®é›† [MCore]
â”‚   â”œâ”€â”€ gpt_dataset.py           â†’ GPT æ•°æ®é›†å¤„ç†
â”‚   â”œâ”€â”€ blended_megatron_dataset_builder.py â†’ æ··åˆæ•°æ®é›†
â”‚   â””â”€â”€ indexed_dataset.py       â†’ ç´¢å¼•æ•°æ®é›†æ ¼å¼
â”‚
â”œâ”€â”€ inference/                   ğŸ† æ¨ç†å¼•æ“ [MCore]
â”‚   â”œâ”€â”€ unified_memory.py        â†’ ç»Ÿä¸€å†…å­˜ç®¡ç†
â”‚   â””â”€â”€ contexts/                â†’ æ¨ç†ä¸Šä¸‹æ–‡
â”‚
â”œâ”€â”€ quantization/                ğŸ† é‡åŒ–æ”¯æŒ [MCore]
â”‚   â”œâ”€â”€ fp8_utils.py             â†’ FP8 ç²¾åº¦å·¥å…·
â”‚   â”œâ”€â”€ fp4_utils.py             â†’ FP4 é‡åŒ–å·¥å…·
â”‚   â””â”€â”€ quant_recipe/            â†’ é‡åŒ–é…ç½®
â”‚
â”œâ”€â”€ ssm/                         ğŸ† çŠ¶æ€ç©ºé—´æ¨¡å‹ [MCore]
â”‚   â”œâ”€â”€ mamba_block.py           â†’ Mamba å—å®ç°
â”‚   â”œâ”€â”€ mamba_layer.py           â†’ Mamba å±‚
â”‚   â”œâ”€â”€ mamba_mixer.py           â†’ Mamba Mixer
â”‚   â”œâ”€â”€ mamba_context_parallel.py â†’ Mamba ä¸Šä¸‹æ–‡å¹¶è¡Œ
â”‚   â”œâ”€â”€ gated_delta_net.py       â†’ Gated Delta Net
â”‚   â””â”€â”€ mlp_layer.py             â†’ MLP å±‚
â”‚
â”œâ”€â”€ tokenizers/                  ğŸ† åˆ†è¯å™¨ [MCore]
â”‚   â”œâ”€â”€ base_tokenizer.py        â†’ åˆ†è¯å™¨åŸºç±»
â”‚   â”œâ”€â”€ megatron_tokenizer.py    â†’ Megatron åˆ†è¯å™¨
â”‚   â””â”€â”€ text/                    â†’ æ–‡æœ¬å¤„ç†
â”‚
â”œâ”€â”€ export/                      ğŸ† æ¨¡å‹å¯¼å‡º [MCore]
â”‚   â”œâ”€â”€ export_config.py         â†’ å¯¼å‡ºé…ç½®
â”‚   â”œâ”€â”€ model_type.py            â†’ æ¨¡å‹ç±»å‹å®šä¹‰
â”‚   â””â”€â”€ trtllm/                  â†’ TensorRT-LLM å¯¼å‡º
â”‚
â”œâ”€â”€ resharding/                  ğŸ† å‚æ•°é‡åˆ†ç‰‡ [MCore]
â”‚   â”œâ”€â”€ refit.py                 â†’ æ¨¡å‹å¾®è°ƒé‡åˆ†ç‰‡
â”‚   â”œâ”€â”€ planner.py               â†’ é‡åˆ†ç‰‡è§„åˆ’å™¨
â”‚   â””â”€â”€ copy_services/           â†’ å¤åˆ¶æœåŠ¡
â”‚
â”œâ”€â”€ post_training/               ğŸ† è®­ç»ƒåå¤„ç† [MCore]
â”‚   â”œâ”€â”€ alignment/               â†’ æ¨¡å‹å¯¹é½ (RLHF)
â”‚   â””â”€â”€ dpo/                     â†’ DPO (Direct Preference Optimization)
â”‚
â”œâ”€â”€ extensions/                  ğŸ† æ‰©å±•åŠŸèƒ½ [MCore]
â”‚   â”‚                           â†’ ç¬¬ä¸‰æ–¹æ‰©å±•æ¥å£
â”‚
â””â”€â”€ parallel_state.py            ğŸ† å¹¶è¡ŒçŠ¶æ€ç®¡ç† [MCore]
```

## å…³é”®å¹¶è¡Œç­–ç•¥è¯¦è§£

### 1. **å¼ é‡å¹¶è¡Œ (Tensor Parallelism, TP)** ğŸ† MCore

**ä½ç½®**ï¼š`megatron/core/tensor_parallel/`

**æ ¸å¿ƒæ–‡ä»¶**ï¼š
- `layers.py` - ColumnParallelLinear, RowParallelLinear
- `mappings.py` - AllGather, ReduceScatter é€šä¿¡åŸè¯­
- `cross_entropy.py` - å¹¶è¡Œè¯æ±‡è¡¨äº¤å‰ç†µ

**å®ç°åŸç†**ï¼š
```
è¾“å…¥: X [batch, seq_len, hidden_size]
         â†“
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚ Column  â”‚  æŒ‰åˆ—åˆ†å‰²æƒé‡ W
    â”‚ Parallelâ”‚  W = [W1, W2, W3, W4]
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â†“ Y = XW (å„GPUç‹¬ç«‹è®¡ç®—)
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚All-Reduceâ”‚  æ±‡æ€»ç»“æœ
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
         â†“ è¾“å‡º
```

**ä»£ç å‚è€ƒ**ï¼š`megatron/core/tensor_parallel/layers.py:89-99`

### 2. **æµæ°´çº¿å¹¶è¡Œ (Pipeline Parallelism, PP)** ğŸ† MCore

**ä½ç½®**ï¼š`megatron/core/pipeline_parallel/`

**æ ¸å¿ƒæ–‡ä»¶**ï¼š
- `schedules.py` - 1F1B, interleaved è°ƒåº¦
- `p2p_communication.py` - é˜¶æ®µé—´é€šä¿¡

**1F1B è°ƒåº¦ç¤ºä¾‹**ï¼š
```
GPU0: F1 â†’ F2 â†’ F3 â†’ B3 â†’ B2 â†’ B1
GPU1:      F1 â†’ F2 â†’ F3 â†’ B3 â†’ B2 â†’ B1
GPU2:           F1 â†’ F2 â†’ F3 â†’ B3 â†’ B2 â†’ B1
GPU3:                F1 â†’ F2 â†’ F3 â†’ B3 â†’ B2 â†’ B1
æ—¶é—´ â†’
```

**ä»£ç å‚è€ƒ**ï¼š`megatron/core/pipeline_parallel/schedules.py:45-145`

### 3. **æ•°æ®å¹¶è¡Œ (Data Parallelism, DP)** ğŸ† MCore

**ä½ç½®**ï¼š`megatron/core/distributed/`

**æ ¸å¿ƒæ–‡ä»¶**ï¼š
- `distributed_data_parallel.py` - DDP åŒ…è£…å™¨
- `torch_fully_sharded_data_parallel.py` - FSDP (ZeRO)

### 4. **åºåˆ—å¹¶è¡Œ (Sequence Parallelism, SP)** ğŸ† MCore

**ä½ç½®**ï¼šé›†æˆåœ¨ `tensor_parallel/mappings.py`

**å®ç°**ï¼šå°†åºåˆ—ç»´åº¦åˆ†å‰²åˆ° TP ranks

**ä»£ç å‚è€ƒ**ï¼š`gather_from_sequence_parallel_region()`, `reduce_scatter_to_sequence_parallel_region()`

### 5. **ä¸Šä¸‹æ–‡å¹¶è¡Œ (Context Parallelism, CP)** ğŸ† MCore

**ä½ç½®**ï¼š`megatron/core/pipeline_parallel/hybrid_cp_schedule.py`

**ç”¨é€”**ï¼šè¶…é•¿åºåˆ—çš„æ³¨æ„åŠ›è®¡ç®—å¹¶è¡ŒåŒ–

## è®­ç»ƒå¾ªç¯æ¶æ„

### ä¸»è®­ç»ƒæµç¨‹

**å…¥å£ç‚¹**ï¼š`megatron/training/training.py:pretrain()`

**å®Œæ•´æµç¨‹**ï¼š
```python
# 1. åˆå§‹åŒ– [Training Framework]
initialize_megatron()
â”œâ”€â”€ åˆ†å¸ƒå¼è¿›ç¨‹ç»„åˆ›å»º
â”œâ”€â”€ æ¨¡å‹å¹¶è¡Œè®¾ç½® (parallel_state.initialize_model_parallel)
â””â”€â”€ éšæœºç§å­åˆå§‹åŒ–

# 2. æ„å»ºæ¨¡å‹ [MCore]
model = model_provider()
â””â”€â”€ GPTModel(megatron/core/models/gpt/gpt_model.py)
    â”œâ”€â”€ LanguageModelEmbedding [MCore]
    â”œâ”€â”€ TransformerBlock [MCore]
    â”‚   â””â”€â”€ TransformerLayer Ã— N [MCore]
    â”‚       â”œâ”€â”€ SelfAttention [MCore]
    â”‚       â”œâ”€â”€ MLP [MCore]
    â”‚       â””â”€â”€ LayerNorm [MCore]
    â””â”€â”€ OutputLayer [MCore]

# 3. æ•°æ®åŠ è½½ [MCore + Training]
data_loader = build_pretraining_data_loader()
â””â”€â”€ BlendedMegatronDatasetBuilder [MCore]
    â””â”€â”€ GPTDataset [MCore]

# 4. å‰å‘/åå‘ä¼ æ’­ [MCore]
forward_backward_func = get_forward_backward_func() [MCore]
â”œâ”€â”€ forward_backward_pipelining_with_interleaving [MCore]
â”‚   â””â”€â”€ schedules.py [MCore]
â””â”€â”€ è¿­ä»£å¾®æ‰¹æ¬¡

# 5. ä¼˜åŒ–å™¨æ­¥éª¤ [MCore]
optimizer.step()
â””â”€â”€ DistributedOptimizer [MCore]
    â”œâ”€â”€ æ¢¯åº¦åŒæ­¥ (all-reduce)
    â””â”€â”€ å‚æ•°æ›´æ–°

# 6. æ£€æŸ¥ç‚¹ä¿å­˜ [Training + MCore]
save_checkpoint()
â”œâ”€â”€ checkpointing.py [Training]
â””â”€â”€ dist_checkpointing/ [MCore]
```

## æ¨¡å‹å®ç°è¯¦è§£

### GPT æ¨¡å‹æ¶æ„ ğŸ† MCore

**æ–‡ä»¶**ï¼š`megatron/core/models/gpt/gpt_model.py:45-267`

**ç±»ç»“æ„**ï¼š
```python
class GPTModel(LanguageModule):
    def __init__(self, config, ...):
        # 1. åµŒå…¥å±‚ [MCore]
        self.embedding = LanguageModelEmbedding(
            config=config,
            vocab_size=vocab_size,
            max_sequence_length=max_sequence_length
        ) [MCore: models/common/embeddings/]

        # 2. æ—‹è½¬ä½ç½®ç¼–ç  [MCore]
        self.rotary_pos_emb = RotaryEmbedding(
            kv_channels=config.kv_channels,
            rotary_percent=rotary_percent
        ) [MCore: models/common/embeddings/rotary_pos_embedding.py]

        # 3. Transformer è§£ç å™¨ [MCore]
        self.decoder = TransformerBlock(
            config=config,
            spec=transformer_layer_spec,
            pre_process=pre_process,
            post_process=post_process
        ) [MCore: transformer/transformer_block.py]

        # 4. è¾“å‡ºå±‚ [MCore]
        self.output_layer = ColumnParallelLinear(
            config.hidden_size,
            vocab_size,
            config=config
        ) [MCore: tensor_parallel/layers.py]

    def forward(self, input_ids, position_ids, ...):
        # é¢„å¤„ç†
        decoder_input = self.embedding(input_ids, position_ids)
        rotary_pos_emb = self.rotary_pos_emb(...)

        # Transformer
        hidden_states = self.decoder(
            hidden_states=decoder_input,
            rotary_pos_emb=rotary_pos_emb,
            ...
        )

        # åå¤„ç†
        logits = self.output_layer(hidden_states)
        loss = self.compute_language_model_loss(labels, logits)
        return loss
```

**ç»„ä»¶å±‚çº§**ï¼š
```
GPTModel [MCore]
â”œâ”€â”€ LanguageModelEmbedding [MCore]
â”‚   â”œâ”€â”€ WordEmbeddings [MCore]
â”‚   â””â”€â”€ PositionEmbeddings [MCore]
â”œâ”€â”€ RotaryEmbedding [MCore]
â”œâ”€â”€ TransformerBlock [MCore]
â”‚   â””â”€â”€ [TransformerLayer Ã— num_layers] [MCore]
â”‚       â”œâ”€â”€ SelfAttention [MCore]
â”‚       â”‚   â”œâ”€â”€ QKV Projection [MCore: tensor_parallel]
â”‚       â”‚   â”œâ”€â”€ Scaled Dot-Product [MCore]
â”‚       â”‚   â””â”€â”€ Output Projection [MCore: tensor_parallel]
â”‚       â”œâ”€â”€ MLP [MCore]
â”‚       â”‚   â”œâ”€â”€ FC1 [MCore: tensor_parallel]
â”‚       â”‚   â”œâ”€â”€ Activation (GELU) [MCore]
â”‚       â”‚   â””â”€â”€ FC2 [MCore: tensor_parallel]
â”‚       â””â”€â”€ LayerNorm (Ã—2) [MCore]
â””â”€â”€ OutputLayer [MCore: tensor_parallel]
```

## å†…å­˜ä¼˜åŒ–æŠ€æœ¯ ğŸ† MCore

### 1. **æ¿€æ´»æ£€æŸ¥ç‚¹ (Activation Checkpointing)**

**å®ç°**ï¼š`megatron/core/transformer/transformer_config.py`

**é…ç½®**ï¼š
```python
config.activation_checkpoint_interval = 1  # æ¯å±‚æ£€æŸ¥ç‚¹
config.num_microbatches_with_partial_activation_checkpoints = 4
```

**æ•ˆæœ**ï¼šå†…å­˜å‡å°‘ ~40%ï¼Œè®¡ç®—æ—¶é—´å¢åŠ  ~15%

### 2. **åºåˆ—å¹¶è¡Œ (Sequence Parallelism)**

**å®ç°**ï¼š`megatron/core/tensor_parallel/mappings.py`

**åŸç†**ï¼š
```
ä¼ ç»Ÿ TP: åºåˆ—åœ¨æ¯ä»½å‰¯æœ¬ä¸Šå®Œæ•´å¤åˆ¶
SP:      åºåˆ—ç»´åº¦è¢«åˆ†å‰²åˆ° TP ranks

æ¿€æ´»å†…å­˜: O(batch Ã— seq Ã— hidden / tp_size)
```

### 3. **ç»†ç²’åº¦æ¿€æ´»å¸è½½**

**æ–‡ä»¶**ï¼š`megatron/core/pipeline_parallel/fine_grained_activation_offload.py`

**ä»£ç å‚è€ƒ**ï¼š`megatron/core/models/gpt/gpt_model.py:437-453`

## ç²¾åº¦æ”¯æŒ ğŸ† MCore

| ç²¾åº¦ç±»å‹ | ä½ç½® | ç”¨é€” |
|---------|------|------|
| FP16 | `transformer_config.py` | æ··åˆç²¾åº¦è®­ç»ƒï¼ˆé»˜è®¤ï¼‰ |
| BF16 | `transformer_config.py` | æ›´ç¨³å®šçš„æ··åˆç²¾åº¦ |
| FP8 | `fp8_utils.py` ğŸ† MCore | H100 GPU ä¼˜åŒ– |
| FP4 | `quantization/fp4_utils.py` ğŸ† MCore | æç«¯é‡åŒ–æ¨ç† |

**FP8 å®ç°**ï¼š
```python
# megatron/core/fp8_utils.py
def correct_amax_history_if_needed(fp8_tensor):
    """FP8 ç¼©æ”¾å› å­ç®¡ç†"""
    # è‡ªåŠ¨ç»´æŠ¤ amax å†å²
    # åŠ¨æ€è°ƒæ•´ç¼©æ”¾å› å­
```

## æ€§èƒ½ä¼˜åŒ– ğŸ† MCore

### 1. **CUDA Graphs**

**ä½ç½®**ï¼š`megatron/core/transformer/cuda_graphs.py`

**é…ç½®**ï¼š
```python
config.cuda_graph_impl = "local"
config.cuda_graph_scope = CudaGraphScope.full_iteration
```

**æ•ˆæœ**ï¼šCPU å¯åŠ¨å¼€é”€å‡å°‘ ~30%

### 2. **èåˆæ ¸**

**ä½ç½®**ï¼š`megatron/core/transformer/fusions/`

**ç±»å‹**ï¼š
- `fused_layer_norm.py` - LayerNorm èåˆ
- `fused_softmax.py` - Softmax + mask èåˆ
- `fused_bias_gelu.py` - Bias + GELU èåˆ

### 3. **Flash Attention**

**é…ç½®**ï¼š
```python
config.flash_attention = True
config.flash_decode = True  # æ¨ç†ä¼˜åŒ–
```

**ä»£ç å‚è€ƒ**ï¼š`megatron/core/transformer/attention.py`

### 4. **æ¢¯åº¦ç´¯ç§¯èåˆ**

**å®ç°**ï¼š`fused_weight_gradient_mlp_cuda` è‡ªå®šä¹‰ CUDA æ ¸

**ä»£ç å‚è€ƒ**ï¼š`megatron/core/tensor_parallel/layers.py:44-48`

## æ•°æ®ç®¡é“ ğŸ† MCore

**ç»„ä»¶**ï¼š
```
BlendedMegatronDatasetBuilder [MCore: datasets/blended_megatron_dataset_builder.py]
â”œâ”€â”€ æ•°æ®é›†æ··åˆï¼ˆæŒ‰æ¯”ä¾‹ï¼‰
â”œâ”€â”€ åˆ†å¸ƒå¼é‡‡æ ·
â””â”€â”€ è¿­ä»£å™¨ç®¡ç†
    â†“
GPTDataset [MCore: datasets/gpt_dataset.py]
â”œâ”€â”€ Tokenization
â”œâ”€â”€ Padding & Masking
â””â”€â”€ æ–‡æ¡£åˆ†å‰²
    â†“
IndexedDataset [MCore: datasets/indexed_dataset.py]
â”œâ”€â”€ å†…å­˜æ˜ å°„æ–‡ä»¶
â””â”€â”€ é«˜æ•ˆéšæœºè®¿é—®
```

## ä¼˜åŒ–å™¨å®ç° ğŸ† MCore

**ä½ç½®**ï¼š`megatron/core/optimizer/`

### åˆ†å¸ƒå¼ä¼˜åŒ–å™¨

**æ–‡ä»¶**ï¼š`distrib_optimizer.py`

**ç‰¹æ€§**ï¼š
- æ¢¯åº¦åˆ†æ¡¶ï¼ˆå‡å°‘é€šä¿¡æ¬¡æ•°ï¼‰
- è®¡ç®—ä¸é€šä¿¡é‡å 
- FP32 ä¸»æƒé‡ç»´æŠ¤
- æ¢¯åº¦è£å‰ª

**é…ç½®**ï¼š
```python
# Adam ä¼˜åŒ–å™¨é…ç½® [MCore]
optimizer_config = AdamOptimizerConfig(
    lr=1e-4,
    betas=(0.9, 0.999),
    eps=1e-8,
    weight_decay=0.01
)
```

## æ£€æŸ¥ç‚¹ä¿å­˜

### åˆ†å¸ƒå¼æ£€æŸ¥ç‚¹ ğŸ† MCore

**ä½ç½®**ï¼š`megatron/core/dist_checkpointing/`

**ç‰¹æ€§**ï¼š
- åˆ†ç‰‡æ£€æŸ¥ç‚¹æ ¼å¼
- å¹¶è¡Œä¿å­˜/åŠ è½½
- æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€
- å®¹é”™æ¢å¤

**Training Framework ç»„ä»¶**ï¼š
- `megatron/training/checkpointing.py` - é«˜çº§æ£€æŸ¥ç‚¹ç®¡ç†

## é…ç½®ç³»ç»Ÿ ğŸ† MCore

### ModelParallelConfig

**æ–‡ä»¶**ï¼š`megatron/core/model_parallel_config.py`

**å‚æ•°**ï¼š
```python
@dataclass
class ModelParallelConfig:
    # å¹¶è¡Œåº¦é…ç½®
    tensor_model_parallel_size: int = 1           # TP åº¦æ•°
    pipeline_model_parallel_size: int = 1          # PP åº¦æ•°
    virtual_pipeline_model_parallel_size: int = None  # äº¤é”™ PP
    sequence_parallel: bool = False                # åºåˆ—å¹¶è¡Œ
    context_parallel_size: int = 1                 # CP åº¦æ•°
    expert_model_parallel_size: int = 1            # MoE ä¸“å®¶å¹¶è¡Œ
```

### TransformerConfig

**æ–‡ä»¶**ï¼š`megatron/core/transformer/transformer_config.py`

**å‚æ•°**ï¼š
```python
@dataclass
class TransformerConfig(ModelParallelConfig):
    # æ¶æ„å‚æ•°
    hidden_size: int = 5120
    num_layers: int = 40
    num_attention_heads: int = 40
    kv_channels: int = 128
    ffn_hidden_size: int = 13696

    # ç²¾åº¦é…ç½®
    fp16: bool = False
    bf16: bool = True
    fp8: str = None  # 'e4m3' or 'hybrid'

    # ä¼˜åŒ–é…ç½®
    apply_layernorm_1p: bool = False
    apply_residual_connection_post_layernorm: bool = False
    ...
```

## æ‰©å±•èƒ½åŠ›

| ç»´åº¦ | è§„æ¨¡ | æŠ€æœ¯æ”¯æŒ |
|-----|------|---------|
| æ¨¡å‹å¤§å° | ä¸‡äº¿çº§å‚æ•° | TP + PP + 3Då¹¶è¡Œ |
| GPU æ•°é‡ | æ•°åƒä¸ª GPU | NCCL é€šä¿¡ä¼˜åŒ– |
| åºåˆ—é•¿åº¦ | è¶…é•¿ä¸Šä¸‹æ–‡ | CP + SP |
| ååé‡ | é«˜åå | CUDA Graphs + èåˆæ ¸ |

## ä¸å…¶ä»–æ¡†æ¶é›†æˆ

### Transformer Engine

**ä½ç½®**ï¼š`megatron/core/tensor_parallel/layers.py:51-56`

```python
try:
    import transformer_engine
    HAVE_TE = True
except ImportError:
    HAVE_TE = False
```

**åŠŸèƒ½**ï¼šFP8 è®­ç»ƒã€èåˆæ“ä½œ

## æµ‹è¯•ä¸éªŒè¯

**æµ‹è¯•å¥—ä»¶**ï¼š`tests/`

- å•å…ƒæµ‹è¯•ï¼š`tests/unit_tests/`
- é›†æˆæµ‹è¯•ï¼š`tests/integration_tests/`
- æ€§èƒ½æµ‹è¯•ï¼š`tests/performance/`

## æ¨ç†æ”¯æŒ ğŸ† MCore

**ä½ç½®**ï¼š`megatron/core/inference/`

**ç‰¹æ€§**ï¼š
- KV ç¼“å­˜
- åŠ¨æ€æ‰¹å¤„ç†
- å¤š GPU æ¨ç†

**æ¨ç†æœåŠ¡å™¨**ï¼š`megatron/inference/`

## å…³é”®è®¾è®¡åŸåˆ™

1. **æ¨¡å—åŒ–**ï¼šMCore å’Œ Training æ¸…æ™°åˆ†ç¦»
2. **æ€§èƒ½ä¼˜å…ˆ**ï¼šGPU ä¼˜åŒ–æ ¸ã€é€šä¿¡é‡å 
3. **å¯æ‰©å±•**ï¼šæ”¯æŒæ•°åƒ GPU
4. **é…ç½®é©±åŠ¨**ï¼šçµæ´»çš„å‚æ•°é…ç½®
5. **ç”Ÿäº§å°±ç»ª**ï¼šå®Œå–„çš„æµ‹è¯•å’Œæ–‡æ¡£

## ä»£ç è´¨é‡

- âœ… ç±»å‹æç¤ºï¼ˆType Hintsï¼‰
- âœ… æ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆDocstringsï¼‰
- âœ… æ—¥å¿—ç³»ç»Ÿï¼ˆLoggingï¼‰
- âœ… é”™è¯¯å¤„ç†ï¼ˆError Handlingï¼‰
- âœ… å‘åå…¼å®¹ï¼ˆBackward Compatibilityï¼‰

## MCore vs Training Framework å¯¹æ¯”

| ç‰¹æ€§ | MCore (megatron/core/) | Training Framework (megatron/training/) |
|-----|------------------------|----------------------------------------|
| **å®šä½** | å¯é‡ç”¨çš„æ ¸å¿ƒåº“ | ç«¯åˆ°ç«¯è®­ç»ƒè„šæœ¬ |
| **é¢å‘å¯¹è±¡** | æ¡†æ¶å¼€å‘è€… | æœ€ç»ˆç”¨æˆ· |
| **ç»„ä»¶ç±»å‹** | æ¨¡å‹ã€å¹¶è¡Œã€ä¼˜åŒ–å™¨ | è®­ç»ƒå¾ªç¯ã€å‚æ•°è§£æ |
| **ç‹¬ç«‹æ€§** | å¯ç‹¬ç«‹ä½¿ç”¨ | ä¾èµ– MCore |
| **ä¿®æ”¹é¢‘ç‡** | ä½ï¼ˆç¨³å®š APIï¼‰ | é«˜ï¼ˆçµæ´»å®éªŒï¼‰ |

## æ–‡ä»¶å‚è€ƒç´¢å¼•

### MCore æ ¸å¿ƒæ–‡ä»¶ ğŸ†

**æ¨¡å‹**ï¼š
- `megatron/core/models/gpt/gpt_model.py:45-267` - GPT æ¨¡å‹
- `megatron/core/models/bert/bert_model.py` - BERT æ¨¡å‹
- `megatron/core/models/t5/t5_model.py` - T5 æ¨¡å‹

**Transformer**ï¼š
- `megatron/core/transformer/attention.py` - æ³¨æ„åŠ›å®ç°
- `megatron/core/transformer/mlp.py` - MLP å®ç°
- `megatron/core/transformer/transformer_block.py` - Transformer å—
- `megatron/core/transformer/transformer_config.py` - é…ç½®ç±»

**å¹¶è¡Œ**ï¼š
- `megatron/core/tensor_parallel/layers.py:89-99` - å¹¶è¡Œå±‚
- `megatron/core/pipeline_parallel/schedules.py:45-145` - è°ƒåº¦å™¨
- `megatron/core/distributed/distributed_data_parallel.py` - DDP

**ä¼˜åŒ–å™¨**ï¼š
- `megatron/core/optimizer/distrib_optimizer.py` - åˆ†å¸ƒå¼ä¼˜åŒ–å™¨
- `megatron/core/optimizer/optimizer.py` - åŸºç¡€ä¼˜åŒ–å™¨

**æ•°æ®**ï¼š
- `megatron/core/datasets/gpt_dataset.py` - GPT æ•°æ®é›†
- `megatron/core/datasets/blended_megatron_dataset_builder.py` - æ•°æ®é›†æ„å»ºå™¨

### Training Framework æ–‡ä»¶

**è®­ç»ƒ**ï¼š
- `megatron/training/training.py:1-200` - ä¸»è®­ç»ƒå¾ªç¯
- `megatron/training/arguments.py` - å‚æ•°è§£æ
- `megatron/training/initialize.py` - åˆå§‹åŒ–
- `megatron/training/checkpointing.py` - æ£€æŸ¥ç‚¹ç®¡ç†

**å¹¶è¡ŒçŠ¶æ€**ï¼š
- `megatron/core/parallel_state.py` - å¹¶è¡ŒçŠ¶æ€ç®¡ç†

---

*åŸºäº Megatron-LM ä»£ç åº“æ·±åº¦åˆ†æ*
*ğŸ† = MCore ç»„ä»¶ï¼Œä½äº megatron/core/ ç›®å½•*
