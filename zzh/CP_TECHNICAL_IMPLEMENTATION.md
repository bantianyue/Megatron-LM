# Megatron-LM Context Parallelism (CP) æŠ€æœ¯å®ç°è¯¦è§£

## ğŸ“Œ å¿«é€Ÿå‚è€ƒï¼ˆåŠé¡µææ–™ï¼‰

> é€‚åˆå¿«é€Ÿäº†è§£ Megatron CP å®ç°è¦ç‚¹

### å®ç°æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Megatron CP = Ring Attention (Transformer Engine å†…éƒ¨)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. è¿›ç¨‹ç»„åˆå§‹åŒ– â†’ åˆ›å»º _CONTEXT_PARALLEL_GROUP           â”‚
â”‚  2. Attention å±‚åˆ›å»º â†’ é…ç½® CP é€šä¿¡ç±»å‹ + CP Stream        â”‚
â”‚  3. Ring Attention â†’ P2P ç¯å½¢é€šä¿¡ (TE å†…éƒ¨å®ç°)           â”‚
â”‚  4. Transformer Layer â†’ CP è‡ªåŠ¨é›†æˆï¼Œç”¨æˆ·é€æ˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å…³é”®ä»£ç ä½ç½®

| æŠ€æœ¯ç‚¹ | æ–‡ä»¶ | è¡Œå· | åŠŸèƒ½ |
|--------|------|------|------|
| **è¿›ç¨‹ç»„ç®¡ç†** | `parallel_state.py` | 972-999 | åˆ›å»º CP è¿›ç¨‹ç»„ |
| **CP Attention** | `transformer_engine.py` | 1141-1250 | TEDotProductAttention |
| **åŠ¨æ€ CP ç»„** | `transformer_engine.py` | 1346-1363 | è¿è¡Œæ—¶åˆ‡æ¢ CP |
| **Layer é›†æˆ** | `transformer_block.py` | - | è‡ªåŠ¨é›†æˆ CP |

### æ ¸å¿ƒä»£ç 

```python
# 1. é…ç½® CP
config = TransformerConfig(
    context_parallel_size=2,    # CP = 2
    cp_comm_type="p2p",         # Ring Attention
)

# 2. åˆ›å»ºæ¨¡å‹ (CP è‡ªåŠ¨é›†æˆ)
model = TransformerBlock(config=config)

# 3. å‰å‘ä¼ æ’­ (CP é€æ˜)
output = model(hidden_states)
```

### Ring Attention é€šä¿¡æµç¨‹

```
CP=2, Seq=8192:
Round 0: Rank 0 è®¡ç®— [0-4095]Ã—[0-4095]
        Rank 1 è®¡ç®— [4096-8191]Ã—[4096-8191]
Round 1: äº¤æ¢ KVï¼Œè®¡ç®—äº¤å‰ attention
AllGather: åˆå¹¶å®Œæ•´è¾“å‡º
```

### å¿«é€Ÿå¯åŠ¨

```bash
export CUDA_DEVICE_MAX_CONNECTIONS=1
torchrun --nproc_per_node=8 pretrain_gpt.py \
    --context-parallel-size 2 \
    --cp-comm-type p2p \
    --seq-length 8192
```

### æ€§èƒ½å¯¹æ¯” (8x A100)

| TP | CP | ååé‡ | æœ€å¤§åºåˆ— | å†…å­˜ |
|:--:|:--:|:------:|:-------:|:---:|
| 8 | 1 | 180 TF | 2048 | 80GB |
| 4 | 2 | 165 TF | 4096 | 40GB |
| 2 | 4 | 140 TF | 8192 | 20GB |

---

## ç›®å½•
- [1. CP æ ¸å¿ƒæ¦‚å¿µ](#1-cp-æ ¸å¿ƒæ¦‚å¿µ)
- [2. å®ç°æ¶æ„](#2-å®ç°æ¶æ„)
- [3. é€šä¿¡æœºåˆ¶](#3-é€šä¿¡æœºåˆ¶)
- [4. ä»£ç å®ç°åˆ†æ](#4-ä»£ç å®ç°åˆ†æ)
- [5. ä½¿ç”¨æŒ‡å—](#5-ä½¿ç”¨æŒ‡å—)
- [6. æ€§èƒ½ä¼˜åŒ–](#6-æ€§èƒ½ä¼˜åŒ–)
- [7. API å‚è€ƒ](#7-api-å‚è€ƒ)
- [8. å®Œæ•´è®­ç»ƒç¤ºä¾‹](#8-å®Œæ•´è®­ç»ƒç¤ºä¾‹)
- [9. å¸¸è§é—®é¢˜](#9-å¸¸è§é—®é¢˜)
- [10. æ€»ç»“](#10-æ€»ç»“)
- [11. å‚è€ƒèµ„æ–™](#11-å‚è€ƒèµ„æ–™)

---

## 1. CP æ ¸å¿ƒæ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯ Context Parallelism

Context Parallelism (CP) æ˜¯ä¸€ç§å°†**åºåˆ—ç»´åº¦**åˆ†å‰²åˆ°å¤šä¸ª GPU ä¸Šçš„å¹¶è¡Œç­–ç•¥ã€‚

**å¯¹æ¯”å…¶ä»–å¹¶è¡Œæ–¹å¼ï¼š**

| å¹¶è¡Œç±»å‹ | åˆ†å‰²ç»´åº¦ | è¯´æ˜ |
|---------|---------|------|
| TP (Tensor Parallel) | Hidden Dimension | åˆ†å‰²æ¨¡å‹æƒé‡ |
| PP (Pipeline Parallel) | Layer | åˆ†å‰²å±‚ |
| DP (Data Parallel) | Batch | åˆ†å‰²æ•°æ® |
| **CP (Context Parallel)** | **Sequence Length** | **åˆ†å‰²åºåˆ—é•¿åº¦** |

### 1.2 CP çš„åº”ç”¨åœºæ™¯

```
é€‚ç”¨åœºæ™¯ï¼š
â”œâ”€â”€ è¶…é•¿åºåˆ—è®­ç»ƒ (seq_length > 8192)
â”œâ”€â”€ é•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹
â”œâ”€â”€ æ–‡æ¡£çº§åˆ«çš„ QA ä»»åŠ¡
â””â”€â”€ å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå›¾æ–‡é•¿åºåˆ—ï¼‰
```

### 1.3 Ring Attention åŸç†

CP ä½¿ç”¨ Ring Attention æ¥å®ç°é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—ï¼š

```
åŸå§‹ Attention: O(NÂ²) å†…å­˜å¤æ‚åº¦
Ring Attention: O(NÂ²/P) å†…å­˜å¤æ‚åº¦ (P=CP size)

Ring é€šä¿¡æ¨¡å¼:
Rank 0: [0,1,2] â†â†’ [3,4,5] â†’ [6,7,8]
Rank 1: [3,4,5] â†â†’ [6,7,8] â†’ [0,1,2]
Rank 2: [6,7,8] â†â†’ [0,1,2] â†’ [3,4,5]
```

---

## 2. å®ç°æ¶æ„

### 2.1 æ ¸å¿ƒæ–‡ä»¶ç»“æ„

```
megatron/core/
â”œâ”€â”€ parallel_state.py              # CP è¿›ç¨‹ç»„ç®¡ç†
â”œâ”€â”€ model_parallel_config.py       # CP é…ç½®å‚æ•°
â”œâ”€â”€ extensions/
â”‚   â””â”€â”€ transformer_engine.py      # TE DotProductAttention CP å®ç°
â”œâ”€â”€ ssm/
â”‚   â””â”€â”€ mamba_context_parallel.py  # Mamba CP å®ç°
â””â”€â”€ models/
    â””â”€â”€ multimodal/
        â””â”€â”€ context_parallel.py    # å¤šæ¨¡æ€ CP å·¥å…·å‡½æ•°
```

### 2.2 è¿›ç¨‹ç»„æ¶æ„

```
World Size (æ€» GPU æ•°)
â”‚
â”œâ”€â”€ Data Parallel Group (DP)
â”‚   â””â”€â”€ åŒ…å« TP Ã— PP Ã— CP
â”‚
â”œâ”€â”€ Tensor Parallel Group (TP)
â”‚   â””â”€â”€ åˆ†å‰²éšè—ç»´åº¦
â”‚
â”œâ”€â”€ Pipeline Parallel Group (PP)
â”‚   â””â”€â”€ åˆ†å‰²å±‚
â”‚
â””â”€â”€ Context Parallel Group (CP) â† CP æ ¸å¿ƒè¿›ç¨‹ç»„
    â””â”€â”€ åˆ†å‰²åºåˆ—é•¿åº¦
```

### 2.3 ç›¸å…³è¿›ç¨‹ç»„

| è¿›ç¨‹ç»„å˜é‡ | è¯´æ˜ |
|-----------|------|
| `_CONTEXT_PARALLEL_GROUP` | ä¸» CP è¿›ç¨‹ç»„ |
| `_CONTEXT_PARALLEL_GLOBAL_RANKS` | CP ç»„å†…æ‰€æœ‰ rank |
| `_HIERARCHICAL_CONTEXT_PARALLEL_GROUPS` | å±‚æ¬¡åŒ– CP è¿›ç¨‹ç»„ |
| `_TENSOR_AND_CONTEXT_PARALLEL_GROUP` | TP+CP ç»„åˆè¿›ç¨‹ç»„ |
| `_DATA_PARALLEL_GROUP_WITH_CP` | DPÃ—CP ç»„åˆè¿›ç¨‹ç»„ |

---

## 3. é€šä¿¡æœºåˆ¶

### 3.1 æ”¯æŒçš„é€šä¿¡ç±»å‹

```python
# megatron/core/extensions/transformer_engine.py:1162
cp_comm_type: Optional[str] = "p2p"  # å››ç§ç±»å‹
```

| ç±»å‹ | å…¨ç§° | é€šä¿¡æ¨¡å¼ | ç‰¹ç‚¹ |
|------|------|---------|------|
| `p2p` | Point-to-Point | Ring Attention | ä½å»¶è¿Ÿï¼Œé€‚åˆé•¿åºåˆ— |
| `a2a` | All-to-All | å…¨å±€äº¤æ¢ | å‡è¡¡è´Ÿè½½ |
| `allgather` | All-Gather | å…¨æ”¶é›† | ç®€å•ä½†å†…å­˜å¼€é”€å¤§ |
| `a2a+p2p` | Hybrid | å±‚æ¬¡åŒ–æ··åˆ | ç»“åˆä¸¤è€…ä¼˜åŠ¿ |

### 3.2 P2P (Ring Attention) æ•°æ®æµ

```
æ­¥éª¤ 1: åˆ†å‰²åºåˆ—
Input: [token_0, token_1, ..., token_8191]
â”œâ”€â”€ CP Rank 0: [token_0, ..., token_4095]
â””â”€â”€ CP Rank 1: [token_4096, ..., token_8191]

æ­¥éª¤ 2: Ring é€šä¿¡ (P2P)
Round 0:
  Rank 0: è®¡ç®— [0-4095] Ã— [0-4095], å‘é€ KV(0-4095) â†’ Rank 1
  Rank 1: è®¡ç®— [4096-8191] Ã— [4096-8191], å‘é€ KV(4096-8191) â†’ Rank 0

Round 1:
  Rank 0: æ¥æ”¶ KV(4096-8191), è®¡ç®— [0-4095] Ã— [4096-8191]
  Rank 1: æ¥æ”¶ KV(0-4095), è®¡ç®— [4096-8191] Ã— [0-4095]

æ­¥éª¤ 3: è¾“å‡º AllGather
Output: [token_0, token_1, ..., token_8191] (å®Œæ•´åºåˆ—)
```

### 3.3 A2A (All-to-All) æ•°æ®æµ

```
A2A å°†åºåˆ—ç»´åº¦å’Œéšè—ç»´åº¦è¿›è¡Œäº¤æ¢ï¼š

è¾“å…¥å½¢çŠ¶: [seq_len/cp_size, batch, hidden]
         â†“ All-to-All
è¾“å‡ºå½¢çŠ¶: [seq_len, batch, hidden/cp_size]

ç”¨äº Mamba ç­‰çŠ¶æ€ç©ºé—´æ¨¡å‹ã€‚
```

### 3.4 å±‚æ¬¡åŒ– CP (a2a+p2p)

```python
# é…ç½®ç¤ºä¾‹
hierarchical_context_parallel_sizes = [4, 2]  # æ€»å…± 8 ä¸ª CP rank

# é€šä¿¡æµç¨‹
# 1. å†…å±‚ç»„ (4 ä¸ª rank) ä½¿ç”¨ A2A é€šä¿¡
# 2. å¤–å±‚ç»„ (2 ä¸ªç»„) ä½¿ç”¨ P2P é€šä¿¡
```

---

## 4. ä»£ç å®ç°åˆ†æ

### 4.0 æ€»ä½“å®ç°æ¦‚è¿°

Megatron-LM çš„ Context Parallelism (CP) å®ç°åŸºäº **Transformer Engine** çš„ Flash Attentionï¼Œä¸»è¦é€šè¿‡ä»¥ä¸‹æœºåˆ¶å®ç°ï¼š

#### æ ¸å¿ƒå®ç°æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CP å®ç°æµç¨‹ (Transformer)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  1. åˆå§‹åŒ–é˜¶æ®µ                                                  â”‚
â”‚     â””â”€â”€ parallel_state.initialize_model_parallel()            â”‚
â”‚         â””â”€â”€ åˆ›å»º _CONTEXT_PARALLEL_GROUP è¿›ç¨‹ç»„                â”‚
â”‚                                                                â”‚
â”‚  2. Attention å±‚åˆ›å»º                                           â”‚
â”‚     â””â”€â”€ TEDotProductAttention.__init__()                       â”‚
â”‚         â”œâ”€â”€ è·å– CP è¿›ç¨‹ç»„                                     â”‚
â”‚         â”œâ”€â”€ è®¾ç½® CP é€šä¿¡ç±»å‹ (p2p/a2a/allgather/a2a+p2p)      â”‚
â”‚         â””â”€â”€ åˆ›å»º CP ä¸“ç”¨ CUDA Stream (é€šä¿¡ä¸è®¡ç®—é‡å )          â”‚
â”‚                                                                â”‚
â”‚  3. å‰å‘ä¼ æ’­                                                    â”‚
â”‚     â””â”€â”€ TEDotProductAttention.forward()                        â”‚
â”‚         â”œâ”€â”€ è¾“å…¥: [seq/cp, batch, heads, head_dim]            â”‚
â”‚         â”œâ”€â”€ Ring Attention é€šä¿¡ (P2P æ¨¡å¼)                     â”‚
â”‚         â”‚   â”œâ”€â”€ Round 0: è®¡ç®—æœ¬åœ° attention                    â”‚
â”‚         â”‚   â”œâ”€â”€ Round 1: å‘é€ KV ç»™é‚»å±…ï¼Œæ¥æ”¶é‚»å±… KV           â”‚
â”‚         â”‚   â””â”€â”€ é‡å¤ CP_size æ¬¡                                â”‚
â”‚         â””â”€â”€ è¾“å‡º: [seq/cp, batch, heads, head_dim]            â”‚
â”‚                                                                â”‚
â”‚  4. åå¤„ç†                                                      â”‚
â”‚     â””â”€â”€ Output Projection + AllGather (å¦‚éœ€è¦)                 â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### å…³é”®æŠ€æœ¯ç‚¹

| æŠ€æœ¯ç‚¹ | è¯´æ˜ | ä»£ç ä½ç½® |
|--------|------|---------|
| **è¿›ç¨‹ç»„ç®¡ç†** | åˆ›å»ºå’Œç®¡ç† CP è¿›ç¨‹ç»„ | `parallel_state.py:972-999` |
| **Ring Attention** | P2P æ¨¡å¼çš„ç¯å½¢é€šä¿¡ | Transformer Engine å†…éƒ¨å®ç° |
| **CP Stream** | ä¸“ç”¨ CUDA Stream å®ç°é€šä¿¡è®¡ç®—é‡å  | `transformer_engine.py:1232-1238` |
| **åŠ¨æ€ CP ç»„** | è¿è¡Œæ—¶åˆ‡æ¢ CP ç»„ | `transformer_engine.py:1346-1363` |
| **Load Balancing** | åºåˆ—é‡æ’åºä¼˜åŒ– | Transformer Engine å†…éƒ¨å®ç° |

#### ä¸ Mamba/Multimodal CP çš„åŒºåˆ«

| æ¨¡å‹ç±»å‹ | CP å®ç°æ–¹å¼ | é€šä¿¡ç±»å‹ |
|---------|-------------|---------|
| **Transformer** | TEDotProductAttention (TE å†…éƒ¨) | Ring Attention (P2P) |
| Mamba | MambaContextParallel (è‡ªå®šä¹‰) | All-to-All |
| Multimodal | è¾…åŠ©å·¥å…·å‡½æ•° | ä»… Padding è®¡ç®— |

**æœ¬æ–‡æ¡£ä»…ä»‹ç» Transformer æ¨¡å‹çš„ CP å®ç°ã€‚**

---

### 4.1 CP è¿›ç¨‹ç»„åˆå§‹åŒ–

**æ–‡ä»¶**: `megatron/core/parallel_state.py:972-999`

```python
def initialize_model_parallel(
    tensor_model_parallel_size: int = 1,
    pipeline_model_parallel_size: int = 1,
    context_parallel_size: int = 1,
    hierarchical_context_parallel_sizes: Optional[list[int]] = None,
    ...
):
    """åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹å¹¶è¡Œè¿›ç¨‹ç»„ï¼ŒåŒ…æ‹¬ CP"""

    global _CONTEXT_PARALLEL_GROUP
    global _CONTEXT_PARALLEL_GLOBAL_RANKS

    # æ„å»º CP è¿›ç¨‹ç»„
    for ranks in decoder_rank_generator.get_ranks('cp'):
        group = create_group(
            ranks,
            timeout=timeout,
            pg_options=get_nccl_options("cp", nccl_comm_cfgs),
            group_desc="CONTEXT_PARALLEL_GROUP",
        )
        if rank in ranks:
            _CONTEXT_PARALLEL_GROUP = group
            _CONTEXT_PARALLEL_GLOBAL_RANKS = ranks

    # å¦‚æœä½¿ç”¨å±‚æ¬¡åŒ– CP
    if hierarchical_context_parallel_sizes:
        global _HIERARCHICAL_CONTEXT_PARALLEL_GROUPS
        hierarchical_groups, _ = create_hierarchical_groups(
            rank,
            ranks,
            hierarchical_context_parallel_sizes,
            create_gloo_process_groups=False,
            pg_options=get_nccl_options("hcp", nccl_comm_cfgs),
            timeout=timeout,
            group_desc="CONTEXT_PARALLEL_GROUP",
        )
        if rank in ranks:
            _HIERARCHICAL_CONTEXT_PARALLEL_GROUPS = hierarchical_groups
```

### 4.2 TE DotProductAttention CP å®ç°

**æ–‡ä»¶**: `megatron/core/extensions/transformer_engine.py:1141-1400`

```python
class TEDotProductAttention(te.pytorch.DotProductAttention):
    """Transformer Engine DotProductAttention çš„ CP åŒ…è£…å™¨"""

    cp_stream: torch.cuda.Stream = None  # CP ä¸“ç”¨ CUDA stream

    def __init__(
        self,
        config: TransformerConfig,
        layer_number: int,
        attn_mask_type: AttnMaskType,
        cp_comm_type: Optional[str] = "p2p",
        pg_collection: Optional[ProcessGroupCollection] = None,
        ...
    ):
        # è·å– CP è¿›ç¨‹ç»„
        if pg_collection is None:
            pg_collection = ProcessGroupCollection(
                tp=get_tensor_model_parallel_group(check_initialized=False),
                cp=get_context_parallel_group(check_initialized=False),
                hcp=get_hierarchical_context_parallel_groups(check_initialized=False),
            )

        # CP é…ç½®
        if self.config.context_parallel_size > 1:
            assert is_te_min_version("1.0.0"), \
                "Only Transformer-Engine version >= 1.0.0 supports context parallelism!"

            # åˆ›å»º CP ä¸“ç”¨ stream
            if getattr(TEDotProductAttention, "cp_stream") is None:
                TEDotProductAttention.cp_stream = torch.cuda.Stream()

            # è®¾ç½® CP å…¨å±€ ranks
            extra_kwargs["cp_global_ranks"] = torch.distributed.get_process_group_ranks(
                pg_collection.cp
            )
            extra_kwargs["cp_stream"] = TEDotProductAttention.cp_stream

            # è®¾ç½® CP é€šä¿¡ç±»å‹
            if is_te_min_version("1.10.0"):
                if cp_comm_type is None:
                    extra_kwargs["cp_comm_type"] = "p2p"
                elif cp_comm_type == "a2a+p2p":
                    assert is_te_min_version("1.12.0"), \
                        "TE >= 1.12.0 required for hierarchical CP"
                    extra_kwargs["cp_comm_type"] = "a2a+p2p"
                    extra_kwargs["cp_group"] = get_hierarchical_context_parallel_groups(
                        check_initialized=False
                    )
                else:
                    extra_kwargs["cp_comm_type"] = cp_comm_type

        # è°ƒç”¨ Transformer Engine åˆå§‹åŒ–
        super().__init__(
            num_attention_heads=config.num_attention_heads,
            num_gqa_groups=config.num_query_groups,
            attn_mask_type=attn_mask_type.value,
            ...
        )

    def forward(
        self,
        query: Tensor,
        key: Tensor,
        value: Tensor,
        attention_mask: Optional[Tensor] = None,
        packed_seq_params: Optional[PackedSeqParams] = None,
        ...
    ):
        """å‰å‘ä¼ æ’­ï¼Œæ”¯æŒåŠ¨æ€ CP ç»„åˆ‡æ¢"""

        # åŠ¨æ€ CP ç»„æ”¯æŒ
        if packed_seq_params is not None:
            if packed_seq_params.cp_group is not None:
                self.cp_group = packed_seq_params.cp_group
                super().set_context_parallel_group(
                    self.cp_group,
                    torch.distributed.get_process_group_ranks(self.cp_group),
                    TEDotProductAttention.cp_stream,
                    self.cp_comm_type,
                )
            # åŠ¨æ€å…³é—­ CP
            elif packed_seq_params.local_cp_size is not None:
                assert packed_seq_params.local_cp_size == 1
                super().set_context_parallel_group(None, None, None, self.cp_comm_type)

        # è°ƒç”¨ TE forward
        return super().forward(
            query, key, value, attention_mask=attention_mask,
            packed_seq_params=packed_seq_params,
        )
```

---

### 4.3 Transformer Layer é›†æˆ

Transformer å±‚é€šè¿‡ `TransformerBlock` è‡ªåŠ¨é›†æˆ CP åŠŸèƒ½ï¼š

**æ–‡ä»¶**: `megatron/core/transformer/transformer_block.py`

```python
class TransformerBlock(MegatronModule):
    """Transformer Blockï¼ŒåŒ…å« Attention å’Œ MLP"""

    def __init__(self, config: TransformerConfig, ...):
        # Attention å±‚ï¼ˆåŒ…å« CP æ”¯æŒï¼‰
        self.self_attention = build_attention(
            config=config,
            layer_number=layer_number,
            attn_mask_type=AttnMaskType.causal,
        )
        # å½“ config.context_parallel_size > 1 æ—¶
        # è‡ªåŠ¨ä½¿ç”¨ TEDotProductAttention

        # MLP å±‚
        self.mlp = MLP(config=config, ...)

    def forward(self, hidden_states, attention_mask, ...):
        # Attention (CP é€šä¿¡åœ¨å†…éƒ¨è‡ªåŠ¨å¤„ç†)
        residual = hidden_states
        hidden_states = self.layer_norm1(hidden_states)
        hidden_states, context = self.self_attention(
            hidden_states,
            attention_mask,
        )
        hidden_states = residual + hidden_states

        # MLP
        residual = hidden_states
        hidden_states = self.layer_norm2(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        return hidden_states, context
```

**å…³é”®ç‚¹**ï¼š
- CP é€šä¿¡å¯¹ç”¨æˆ·é€æ˜ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒç”¨
- åªéœ€åœ¨ `TransformerConfig` ä¸­è®¾ç½® `context_parallel_size`
- Attention å’Œ MLP è‡ªåŠ¨é€‚é… CP æ¨¡å¼

---

## 5. ä½¿ç”¨æŒ‡å—

### 5.1 åŸºæœ¬ä½¿ç”¨

#### å‘½ä»¤è¡Œå‚æ•°

```bash
# CP=2, TP=4, PP=1 çš„é…ç½®
python pretrain_gpt.py \
    --tensor-model-parallel-size 4 \
    --pipeline-model-parallel-size 1 \
    --context-parallel-size 2 \
    --cp-comm-type p2p \
    --seq-length 8192 \
    --micro-batch-size 1 \
    --global-batch-size 64
```

#### Python API

##### ç¤ºä¾‹ 1: åŸºæœ¬ä½¿ç”¨

```python
import torch
from megatron.core.transformer.transformer_config import TransformerConfig
from megatron.core.extensions.transformer_engine import TEDotProductAttention
from megatron.core.transformer.enums import AttnMaskType
from megatron.core import parallel_state
from megatron.core.packed_seq_params import PackedSeqParams

# 1. é…ç½®
config = TransformerConfig(
    hidden_size=4096,
    num_attention_heads=32,
    kv_channels=128,
    # CP é…ç½®
    context_parallel_size=2,
    # å…¶ä»–å¹¶è¡Œé…ç½®
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    sequence_parallel=True,
)

# 2. åˆ›å»º Attention å±‚
attention = TEDotProductAttention(
    config=config,
    layer_number=1,
    attn_mask_type=AttnMaskType.causal,
    cp_comm_type="p2p",
)

# 3. è·å– CP ä¿¡æ¯
cp_group = parallel_state.get_context_parallel_group()
cp_size = parallel_state.get_context_parallel_world_size()
cp_rank = parallel_state.get_context_parallel_rank()

print(f"CP size: {cp_size}, CP rank: {cp_rank}")

# 4. å‡†å¤‡è¾“å…¥æ•°æ®
# è¾“å…¥å½¢çŠ¶: [seq_len, batch, num_heads, head_dim]
# æ³¨æ„ï¼šç”±äº CP åˆ†å‰²åºåˆ—ï¼Œæ¯ä¸ª rank çš„ seq_len = total_seq_len / cp_size
seq_len_per_rank = 2048  # æ€»åºåˆ— 4096 / CP=2
batch_size = 2
num_heads = 32
head_dim = 128

query = torch.randn(seq_len_per_rank, batch_size, num_heads, head_dim, device='cuda')
key = torch.randn(seq_len_per_rank, batch_size, num_heads, head_dim, device='cuda')
value = torch.randn(seq_len_per_rank, batch_size, num_heads, head_dim, device='cuda')

# 5. è°ƒç”¨ forward
context, _ = attention(
    query=query,
    key=key,
    value=value,
    attention_mask=None,  # å› æœæ©ç ç”± attn_mask_type å‚æ•°å¤„ç†
)

# 6. è¾“å‡º
# context å½¢çŠ¶: [seq_len_per_rank, batch, num_heads, head_dim]
print(f"Output shape: {context.shape}")
```

##### ç¤ºä¾‹ 2: ä½¿ç”¨ Packed Sequence (å˜é•¿åºåˆ—)

```python
import torch
from megatron.core.packed_seq_params import PackedSeqParams

# åˆ›å»º PackedSeqParams (ç”¨äºå¤„ç†å˜é•¿åºåˆ—)
packed_seq_params = PackedSeqParams(
    cu_seqlens_q=torch.tensor([0, 100, 200], dtype=torch.int32, device='cuda'),
    cu_seqlens_kv=torch.tensor([0, 100, 200], dtype=torch.int32, device='cuda'),
    cu_seqlens_q_padded=torch.tensor([0, 128, 256], dtype=torch.int32, device='cuda'),
    cu_seqlens_kv_padded=torch.tensor([0, 128, 256], dtype=torch.int32, device='cuda'),
    max_seqlen_q=128,
    max_seqlen_kv=128,
    qkv_format='thd',  # THD æ ¼å¼æ”¯æŒ CP
)

# è°ƒç”¨ forward (ä½¿ç”¨ packed sequence)
context, _ = attention(
    query=query,
    key=key,
    value=value,
    attention_mask=None,
    packed_seq_params=packed_seq_params,  # ä¼ å…¥ packed_seq_params
)
```

##### ç¤ºä¾‹ 3: å®Œæ•´çš„ Transformer Layer ä½¿ç”¨

```python
import torch
import torch.nn as nn
from megatron.core.transformer.transformer_config import TransformerConfig
from megatron.core.transformer.transformer_layer import TransformerLayer
from megatron.core import parallel_state

# 1. åˆå§‹åŒ–å¹¶è¡Œç¯å¢ƒ
# (é€šå¸¸åœ¨è®­ç»ƒè„šæœ¬å¼€å§‹æ—¶è°ƒç”¨ä¸€æ¬¡)
# parallel_state.initialize_model_parallel(...)

# 2. åˆ›å»ºé…ç½®
config = TransformerConfig(
    hidden_size=4096,
    num_attention_heads=32,
    num_layers=24,
    ffn_hidden_size=13696,
    # CP é…ç½®
    context_parallel_size=2,
    tensor_model_parallel_size=4,
    pipeline_model_parallel_size=1,
    sequence_parallel=True,
    # å…¶ä»–é…ç½®
    add_bias_linear=False,
    gated_linear_unit=True,
    activation_func=torch.nn.functional.silu,
    normalization="RMSNorm",
)

# 3. åˆ›å»º Transformer Layer
layer = TransformerLayer(
    config=config,
    layer_number=1,
    hidden_dropout=None,
)

# 4. å‡†å¤‡è¾“å…¥
# å½¢çŠ¶: [seq_len_per_rank, batch, hidden_size]
hidden_states = torch.randn(2048, 2, 4096, device='cuda')

# 5. å‰å‘ä¼ æ’­
# CP é€šä¿¡åœ¨ layer å†…éƒ¨è‡ªåŠ¨å¤„ç†
hidden_states, context = layer(
    hidden_states=hidden_states,
    attention_mask=None,
)

print(f"Output shape: {hidden_states.shape}")
# è¾“å‡º: [2048, 2, 4096] (åºåˆ—é•¿åº¦è¢« CP åˆ†å‰²)
```

##### ç¤ºä¾‹ 4: åŠ¨æ€åˆ‡æ¢ CP ç»„

```python
import torch
from megatron.core.packed_seq_params import PackedSeqParams

# åœºæ™¯: ç¼–ç å™¨ä¸éœ€è¦ CPï¼Œè§£ç å™¨éœ€è¦ CP

# 1. å…³é—­ CP (ç”¨äºç¼–ç å™¨)
packed_seq_params_encoder = PackedSeqParams(
    cu_seqlens_q=torch.tensor([0, 100], dtype=torch.int32, device='cuda'),
    cu_seqlens_kv=torch.tensor([0, 100], dtype=torch.int32, device='cuda'),
    max_seqlen_q=100,
    max_seqlen_kv=100,
    qkv_format='thd',
    local_cp_size=1,  # è®¾ç½®ä¸º 1 è¡¨ç¤ºå…³é—­ CP
)

context_encoder, _ = attention(
    query=query,
    key=key,
    value=value,
    packed_seq_params=packed_seq_params_encoder,
)

# 2. å¯ç”¨ CP (ç”¨äºè§£ç å™¨)
# ä½¿ç”¨æŒ‡å®šçš„ CP ç»„
cp_group = parallel_state.get_context_parallel_group()
packed_seq_params_decoder = PackedSeqParams(
    cu_seqlens_q=torch.tensor([0, 2048], dtype=torch.int32, device='cuda'),
    cu_seqlens_kv=torch.tensor([0, 2048], dtype=torch.int32, device='cuda'),
    max_seqlen_q=2048,
    max_seqlen_kv=2048,
    qkv_format='thd',
    cp_group=cp_group,  # æŒ‡å®š CP ç»„
)

context_decoder, _ = attention(
    query=query,
    key=key,
    value=value,
    packed_seq_params=packed_seq_params_decoder,
)
```

### 5.2 ä¸åŒé€šä¿¡ç±»å‹

#### P2P (Ring Attention) - æ¨è

P2P æ˜¯ Transformer æ¨¡å‹çš„é»˜è®¤é€‰æ‹©ï¼Œä½¿ç”¨ Ring Attention æœºåˆ¶ã€‚

```bash
python pretrain_gpt.py \
    --context-parallel-size 4 \
    --cp-comm-type p2p \
    --seq-length 32768
```

**ç‰¹ç‚¹ï¼š**
- ä½å»¶è¿Ÿï¼Œé€‚åˆé•¿åºåˆ—
- é€šä¿¡å¼€é”€å‡åŒ€åˆ†å¸ƒ
- é€‚ç”¨äºå¤§å¤šæ•° Transformer æ¨¡å‹

#### A2A (All-to-All)

A2A é€šä¿¡æ¨¡å¼ï¼Œé€šè¿‡ all-to-all é›†ä½“é€šä¿¡åŸè¯­å®ç°ã€‚

```bash
python pretrain_gpt.py \
    --context-parallel-size 4 \
    --cp-comm-type a2a \
    --seq-length 16384
```

**ç‰¹ç‚¹ï¼š**
- å‡è¡¡è´Ÿè½½
- é€‚ç”¨äºç‰¹å®šåœºæ™¯

#### å±‚æ¬¡åŒ– CP (a2a+p2p)

```bash
# CP=8, å†…å±‚ A2A group=4, å¤–å±‚ P2P group=2
python pretrain_gpt.py \
    --context-parallel-size 8 \
    --hierarchical-context-parallel-sizes 4 2 \
    --cp-comm-type a2a+p2p \
    --seq-length 65536
```

### 5.3 å¤šå±‚é€šä¿¡ç±»å‹

```bash
# ä¸ºä¸åŒå±‚è®¾ç½®ä¸åŒçš„ CP é€šä¿¡ç±»å‹
python pretrain_gpt.py \
    --context-parallel-size 8 \
    --num-layers 24 \
    --cp-comm-type p2p p2p a2a a2a a2a+p2p a2a+p2p

# å‰ 2 å±‚ä½¿ç”¨ p2p
# æ¥ä¸‹æ¥ 2 å±‚ä½¿ç”¨ a2a
# æœ€å 2 å±‚ä½¿ç”¨ a2a+p2p
# (ä¼šå¾ªç¯åº”ç”¨åˆ°æ‰€æœ‰ 24 å±‚)
```

### 5.4 æ··åˆ CP (å˜é•¿åºåˆ—)

```bash
python pretrain_gpt.py \
    --context-parallel-size 4 \
    --hybrid-context-parallel \
    --max-seqlen-per-dp-cp-rank 8192 \
    --calculate-per-token-loss \
    --dataloader-type single
```

---

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 åºåˆ—é•¿åº¦å¯¹é½

CP å¯¹åºåˆ—é•¿åº¦æœ‰å¯¹é½è¦æ±‚ï¼Œéœ€è¦ç¡®ä¿åºåˆ—é•¿åº¦æ»¡è¶³ CP çš„å€æ•°è¦æ±‚ã€‚

**åŸºæœ¬è§„åˆ™ï¼š**
```python
# åºåˆ—é•¿åº¦å¿…é¡»æ»¡è¶³
seq_length % (2 * context_parallel_size) == 0

# å¦‚æœåŒæ—¶ä½¿ç”¨ Sequence Parallel (SP)
seq_length % (2 * context_parallel_size * tensor_parallel_size) == 0
```

**ç¤ºä¾‹ï¼š**
```python
# é…ç½®: CP=2, TP=4, SP=True
cp_size = 2
tp_size = 4
has_sp = True

# è®¡ç®—æ‰€éœ€ padding
if has_sp and cp_size > 1:
    padding_factor = tp_size * cp_size * 2  # 4 * 2 * 2 = 16
elif cp_size > 1:
    padding_factor = cp_size * 2  # 2 * 2 = 4
elif has_sp:
    padding_factor = tp_size  # 4

# åŸå§‹åºåˆ—é•¿åº¦
seq_len = 5000
padding = padding_factor - (seq_len % padding_factor)  # 16 - 8 = 8
final_seq_len = seq_len + padding  # 5008

# éªŒè¯
assert final_seq_len % padding_factor == 0  # 5008 % 16 == 0 âœ“
```

### 6.2 CP ä¸“ç”¨ Stream

```python
# megatron/core/extensions/transformer_engine.py:1232-1238
if self.config.context_parallel_size > 1:
    # åˆ›å»º CP ä¸“ç”¨ CUDA stream ä»¥å®ç°é€šä¿¡ä¸è®¡ç®—é‡å 
    if getattr(TEDotProductAttention, "cp_stream") is None:
        TEDotProductAttention.cp_stream = torch.cuda.Stream()

    extra_kwargs["cp_stream"] = TEDotProductAttention.cp_stream
```

### 6.3 å†…å­˜ä¼˜åŒ–

| ä¼˜åŒ–æŠ€æœ¯ | è¯´æ˜ |
|---------|------|
| Activation Checkpointing | å‡å°‘æ¿€æ´»å€¼å†…å­˜å ç”¨ |
| Selective Recomputation | é€‰æ‹©æ€§é‡è®¡ç®— |
| Flash Attention | æ³¨æ„åŠ›è®¡ç®—ä¼˜åŒ– |
| FP8 Quantization | æ··åˆç²¾åº¦è®­ç»ƒ |

### 6.4 æ€§èƒ½åŸºå‡†

```
é…ç½®: 8x A100-80GB, GPT-3 175B

TP=8, PP=1, CP=1:
- ååé‡: 180 TFLOPS
- æœ€å¤§åºåˆ—é•¿åº¦: 2048

TP=4, PP=1, CP=2:
- ååé‡: 165 TFLOPS (-8%)
- æœ€å¤§åºåˆ—é•¿åº¦: 4096 (2x)

TP=2, PP=1, CP=4:
- ååé‡: 140 TFLOPS (-22%)
- æœ€å¤§åºåˆ—é•¿åº¦: 8192 (4x)
```

### 6.5 æœ€ä½³å®è·µ

1. **é€‰æ‹©åˆé€‚çš„é€šä¿¡ç±»å‹**
   - è¶…é•¿åºåˆ— (>32K): ä½¿ç”¨ `p2p` (Ring Attention)
   - å¤§è§„æ¨¡ CP (>8): è€ƒè™‘ `a2a+p2p` (å±‚æ¬¡åŒ– CP)
   - ä¸€èˆ¬åœºæ™¯: ä½¿ç”¨ `p2p` å³å¯

2. **CP ä¸å…¶ä»–å¹¶è¡Œçš„ç»„åˆ**
   ```
   æ¨è: TP=4, PP=1, CP=2 (æ€»å…± 8 GPUs)
   é¿å…: CP è¿‡å¤§å¯¼è‡´é€šä¿¡å¼€é”€å¢åŠ 

   å…¬å¼: total_gpus = TP Ã— PP Ã— CP Ã— DP
   ```

3. **åºåˆ—é•¿åº¦è®¾ç½®**
   ```python
   # åºåˆ—é•¿åº¦å¿…é¡»æ»¡è¶³
   seq_length % (2 * context_parallel_size) == 0

   # æ¨è
   seq_length = 8192, cp_size = 2  # OK
   seq_length = 8192, cp_size = 4  # OK
   seq_length = 8000, cp_size = 2  # é”™è¯¯ï¼
   ```

4. **ç¯å¢ƒå˜é‡è®¾ç½®**
   ```bash
   # Hopper åŠä¹‹å‰æ¶æ„
   export CUDA_DEVICE_MAX_CONNECTIONS=1

   # NCCL ä¼˜åŒ–
   export NCCL_ALGO=Tree
   export NCCL_PROTO=Simple
   ```

---

## 7. API å‚è€ƒ

### 7.1 è¿›ç¨‹ç»„ API

```python
# megatron/core/parallel_state.py

def get_context_parallel_group(check_initialized=True):
    """è·å– CP è¿›ç¨‹ç»„"""

def get_context_parallel_world_size(check_initialized=True):
    """è·å– CP å¹¶è¡Œåº¦"""

def get_context_parallel_rank(check_initialized=True):
    """è·å–å½“å‰ rank åœ¨ CP ç»„å†…çš„ç¼–å·"""

def get_context_parallel_global_ranks(check_initialized=True):
    """è·å– CP ç»„å†…æ‰€æœ‰å…¨å±€ rank"""

def get_hierarchical_context_parallel_groups(check_initialized=True):
    """è·å–å±‚æ¬¡åŒ– CP è¿›ç¨‹ç»„"""
```

### 7.2 é…ç½® API

```python
# megatron/core/model_parallel_config.py

@dataclass
class ModelParallelConfig:
    context_parallel_size: int = 1
    hierarchical_context_parallel_sizes: Optional[list[int]] = None
    max_seqlen_per_dp_cp_rank: Optional[int] = None
    hybrid_context_parallel: bool = False
```

### 7.3 Attention Layer API

```python
# megatron/core/extensions/transformer_engine.py

class TEDotProductAttention(te.pytorch.DotProductAttention):
    """Transformer Engine DotProductAttention with CP Support"""

    def __init__(
        self,
        config: TransformerConfig,
        layer_number: int,
        attn_mask_type: AttnMaskType,
        cp_comm_type: Optional[str] = "p2p",  # CP é€šä¿¡ç±»å‹
        pg_collection: Optional[ProcessGroupCollection] = None,
    ):
        ...

    def forward(
        self,
        query: Tensor,
        key: Tensor,
        value: Tensor,
        attention_mask: Optional[Tensor] = None,
        packed_seq_params: Optional[PackedSeqParams] = None,  # æ”¯æŒ packed sequence
    ) -> Tuple[Tensor, Tensor]:
        """å‰å‘ä¼ æ’­ï¼Œè‡ªåŠ¨å¤„ç† CP é€šä¿¡"""
        ...
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from megatron.core.extensions.transformer_engine import TEDotProductAttention

# åˆ›å»º CP Attention å±‚
attention = TEDotProductAttention(
    config=config,
    layer_number=1,
    attn_mask_type=AttnMaskType.causal,
    cp_comm_type="p2p",  # Ring Attention
)

# è°ƒç”¨ forward - CP é€šä¿¡è‡ªåŠ¨å¤„ç†
context, _ = attention(
    query=query,
    key=key,
    value=value,
    attention_mask=None,
)
```

---

## 8. å®Œæ•´è®­ç»ƒç¤ºä¾‹

### 8.1 ç¤ºä¾‹ 1: ä½¿ç”¨ Megatron-LM é¢„è®­ç»ƒè„šæœ¬

#### ç¯å¢ƒå‡†å¤‡

```bash
# 1. è®¾ç½®ç¯å¢ƒå˜é‡ (å¿…éœ€)
export CUDA_DEVICE_MAX_CONNECTIONS=1  # CP å¿…éœ€

# 2. è®¾ç½®åˆ†å¸ƒå¼ç›¸å…³
export MASTER_ADDR=localhost
export MASTER_PORT=6000

# 3. é…ç½® NCCL (å¯é€‰ï¼Œç”¨äºæ€§èƒ½ä¼˜åŒ–)
export NCCL_ALGO=Tree
export NCCL_PROTO=Simple
```

#### å¯åŠ¨è®­ç»ƒè„šæœ¬ (torchrun)

```bash
#!/bin/bash
# launch_cp_training.sh

# é…ç½®
GPUS_PER_NODE=8
NNODES=1
TP=4
PP=1
CP=2

# è®­ç»ƒå‚æ•°
MODEL_SIZE="7B"
SEQ_LEN=8192
GLOBAL_BATCH=64
MICRO_BATCH=1

# å¯åŠ¨è®­ç»ƒ
torchrun --nproc_per_node=$GPUS_PER_NODE \
    --nnodes=$NNODES \
    --master_addr=$MASTER_ADDR \
    --master_port=$MASTER_PORT \
    pretrain_gpt.py \
    --tensor-model-parallel-size $TP \
    --pipeline-model-parallel-size $PP \
    --context-parallel-size $CP \
    --cp-comm-type p2p \
    --sequence-parallel \
    --num-layers 32 \
    --hidden-size 4096 \
    --num-attention-heads 32 \
    --ffn-hidden-size 13696 \
    --seq-length $SEQ_LEN \
    --max-position-embeddings $SEQ_LEN \
    --micro-batch-size $MICRO_BATCH \
    --global-batch-size $GLOBAL_BATCH \
    --train-iters 500000 \
    --lr 1.0e-4 \
    --min-lr 1.0e-5 \
    --lr-decay-style cosine \
    --weight-decay 0.1 \
    --clip-grad 1.0 \
    --adam-beta1 0.9 \
    --adam-beta2 0.95 \
    --init-method-std 0.01 \
    --bf16 \
    --save /path/to/checkpoints \
    --data-path /path/to/data/gpt2_text_document \
    --vocab-file /path/to/gpt2/vocab.json \
    --merge-file /path/to/gpt2/merges.txt
```

#### Slurm é›†ç¾¤å¯åŠ¨

```bash
#!/bin/bash
#SBATCH --job-name=megatron-cp-training
#SBATCH --nodes=4
#SBATCH --ntasks-per-node=8
#SBATCH --gres=gpu:8

MASTER_ADDR=$(scontrol show hostname $SLURM_JOB_NODELIST | head -n 1)
export CUDA_DEVICE_MAX_CONNECTIONS=1

srun bash -c "
    torchrun --nproc_per_node=8 \
        pretrain_gpt.py \
        --tensor-model-parallel-size 4 \
        --context-parallel-size 2 \
        --cp-comm-type p2p \
        --sequence-parallel \
        --seq-length 8192 \
        --bf16
"
```

### 8.2 ç¤ºä¾‹ 2: è‡ªå®šä¹‰è®­ç»ƒè„šæœ¬ (MCore API)

```python
#!/usr/bin/env python3
"""
CP Ring Attention è‡ªå®šä¹‰è®­ç»ƒç¤ºä¾‹
ä½¿ç”¨ Megatron Core API æ„å»º GPT æ¨¡å‹
"""

import os
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

from megatron.core import parallel_state, tensor_parallel
from megatron.core.transformer.transformer_config import TransformerConfig
from megatron.core.models.gpt.gpt_model import GPTModel
from megatron.training import get_args, get_timers
from megatron.training.initialize import initialize_megatron


# ============== æ•°æ®é›† ==============
class SimpleTextDataset(Dataset):
    """ç®€å•çš„æ–‡æœ¬æ•°æ®é›†"""

    def __init__(self, seq_length=8192, num_samples=10000, vocab_size=50257):
        self.seq_length = seq_length
        self.num_samples = num_samples
        self.vocab_size = vocab_size

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        tokens = torch.randint(0, self.vocab_size, (self.seq_length,))
        labels = tokens.clone()
        return tokens, labels


# ============== æ¨¡å‹é…ç½® ==============
def get_model_config():
    """è·å–æ¨¡å‹é…ç½®"""
    args = get_args()

    config = TransformerConfig(
        # æ¨¡å‹æ¶æ„
        num_layers=args.num_layers,
        hidden_size=args.hidden_size,
        num_attention_heads=args.num_attention_heads,
        ffn_hidden_size=args.ffn_hidden_size,

        # CP é…ç½®
        tensor_model_parallel_size=args.tensor_model_parallel_size,
        pipeline_model_parallel_size=args.pipeline_model_parallel_size,
        context_parallel_size=args.context_parallel_size,
        sequence_parallel=args.sequence_parallel,

        # CP é€šä¿¡ç±»å‹
        cp_comm_type=getattr(args, 'cp_comm_type', 'p2p'),

        # å…¶ä»–é…ç½®
        add_bias_linear=False,
        gated_linear_unit=True,
        activation_func=F.silu,
        normalization="RMSNorm",

        # ç²¾åº¦
        bf16=args.bf16,
        params_dtype=torch.bfloat16,
    )

    return config


# ============== æ¨¡å‹å®šä¹‰ ==============
def model_provider():
    """è¿”å›æ¨¡å‹æä¾›å‡½æ•°"""
    config = get_model_config()

    model = GPTModel(
        config=config,
        transformer_config=config,
        vocab_size=args.padded_vocab_size,
        max_sequence_length=args.seq_length,
        parallel_output=True,
    )

    return model


# ============== è®­ç»ƒå¾ªç¯ ==============
def train_epoch(model, optimizer, lr_scheduler, dataloader, epoch):
    """è®­ç»ƒä¸€ä¸ª epoch"""
    model.train()

    for step, (tokens, labels) in enumerate(dataloader):
        tokens = tokens.cuda(non_blocking=True)
        labels = labels.cuda(non_blocking=True)

        # å‰å‘ä¼ æ’­ - CP é€šä¿¡åœ¨æ¨¡å‹å†…éƒ¨è‡ªåŠ¨å¤„ç†
        logits = model(tokens)

        # ä»…åœ¨ pipeline æœ€å stage è®¡ç®—æŸå¤±
        if parallel_state.is_pipeline_last_stage():
            # Tensor Parallel vocab åˆ†åŒº
            logits = tensor_parallel.vocab_parallel_with_logits(logits)

            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                labels.view(-1),
                ignore_index=-1,
            )
        else:
            loss = torch.tensor(0.0, device='cuda')

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        if args.clip_grad > 0.0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)

        # æ›´æ–°å‚æ•°
        optimizer.step()
        lr_scheduler.step()

        # æ—¥å¿—
        if step % args.log_interval == 0 and parallel_state.is_rank_0():
            lr = optimizer.param_groups[0]['lr']
            print(f"Epoch {epoch}, Step {step}, Loss: {loss.item():.4f}, LR: {lr:.6f}")


# ============== ä¸»å‡½æ•° ==============
def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # åˆå§‹åŒ– Megatron
    initialize_megatron(
        extra_args_provider=None,
        args_defaults={
            'micro_batch_size': 1,
            'global_batch_size': 64,
            'seq_length': 8192,
            'num_layers': 32,
            'hidden_size': 4096,
            'num_attention_heads': 32,
            'ffn_hidden_size': 13696,
            'tensor_model_parallel_size': 4,
            'pipeline_model_parallel_size': 1,
            'context_parallel_size': 2,
            'cp_comm_type': 'p2p',
            'sequence_parallel': True,
            'bf16': True,
            'train_iters': 500000,
            'lr': 1.0e-4,
            'weight_decay': 0.1,
            'clip_grad': 1.0,
            'log_interval': 1,
        }
    )

    global args
    args = get_args()

    # æ‰“å° CP é…ç½®
    if parallel_state.is_rank_0():
        print("=" * 80)
        print("Context Parallelism Training")
        print("=" * 80)
        print(f"TP: {parallel_state.get_tensor_model_parallel_world_size()}")
        print(f"PP: {parallel_state.get_pipeline_model_parallel_world_size()}")
        print(f"CP: {parallel_state.get_context_parallel_world_size()}")
        print(f"CP comm type: {args.cp_comm_type}")
        print(f"Sequence length: {args.seq_length}")
        print("=" * 80)

    # åˆ›å»ºæ¨¡å‹
    model = model_provider()

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=args.lr,
        betas=(args.adam_beta1, args.adam_beta2),
        weight_decay=args.weight_decay,
    )

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    total_steps = args.train_iters
    warmup_steps = int(total_steps * 0.01)

    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        else:
            from math import cos, pi
            progress = (step - warmup_steps) / (total_steps - warmup_steps)
            return 0.5 * (1.0 + cos(progress * pi))

    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

    # åˆ›å»ºæ•°æ®é›†
    dataset = SimpleTextDataset(
        seq_length=args.seq_length,
        num_samples=args.global_batch_size * 1000,
        vocab_size=args.padded_vocab_size
    )

    sampler = torch.utils.data.distributed.DistributedSampler(
        dataset,
        num_replicas=parallel_state.get_data_parallel_world_size(),
        rank=parallel_state.get_data_parallel_rank(),
        shuffle=True,
    )

    dataloader = DataLoader(
        dataset,
        batch_size=args.micro_batch_size,
        sampler=sampler,
        num_workers=4,
        pin_memory=True,
    )

    # è®­ç»ƒå¾ªç¯
    total_epochs = 10

    for epoch in range(total_epochs):
        if parallel_state.is_rank_0():
            print(f"\n{'='*80}\nEpoch {epoch + 1}/{total_epochs}\n{'='*80}\n")

        train_epoch(model, optimizer, lr_scheduler, dataloader, epoch)

        # ä¿å­˜ checkpoint
        if parallel_state.is_rank_0():
            checkpoint_path = f"./checkpoints/cp_model_epoch_{epoch}.pt"
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
            }, checkpoint_path)
            print(f"Checkpoint saved: {checkpoint_path}")

    if parallel_state.is_rank_0():
        print("\nTraining completed!")


if __name__ == "__main__":
    main()
```

#### å¯åŠ¨è‡ªå®šä¹‰è®­ç»ƒ

```bash
#!/bin/bash
export CUDA_DEVICE_MAX_CONNECTIONS=1

torchrun --nproc_per_node=8 \
    custom_cp_training.py \
    --tensor-model-parallel-size 4 \
    --context-parallel-size 2 \
    --cp-comm-type p2p \
    --sequence-parallel \
    --seq-length 8192
```

### 8.3 ç¤ºä¾‹ 3: æœ€å°åŒ– CP æ¼”ç¤º

```python
#!/usr/bin/env python3
"""
æœ€å°åŒ– CP Ring Attention æ¼”ç¤º
"""
import os
os.environ.setdefault("CUDA_DEVICE_MAX_CONNECTIONS", "1")

import torch
from megatron.core import parallel_state
from megatron.core.transformer.transformer_config import TransformerConfig
from megatron.core.transformer.transformer_block import TransformerBlock


def demo_cp_attention():
    """æ¼”ç¤º CP Attention çš„åŸºæœ¬ä½¿ç”¨"""

    rank = torch.distributed.get_rank() if torch.distributed.is_initialized() else 0
    device = torch.device(f'cuda:{rank}')

    # é…ç½®: TP=2, CP=4 (å‡è®¾ 8 GPUs)
    config = TransformerConfig(
        num_layers=2,
        hidden_size=512,
        num_attention_heads=8,
        ffn_hidden_size=2048,

        # CP é…ç½®
        tensor_model_parallel_size=2,
        context_parallel_size=4,
        sequence_parallel=True,
        cp_comm_type='p2p',

        # ç²¾åº¦
        bf16=True,
    )

    # åˆ›å»º Transformer Block
    transformer_block = TransformerBlock(
        config=config,
        pre_process=True,
        post_process=True,
    ).to(device)

    print(f"Rank {rank}: TP={parallel_state.get_tensor_model_parallel_world_size()}, "
          f"CP={parallel_state.get_context_parallel_world_size()}")

    # å‡†å¤‡è¾“å…¥: ç”±äº CP=4ï¼Œåºåˆ—è¢«åˆ†å‰²
    seq_len_per_rank = 128  # æ€»åºåˆ— 512 / 4
    batch_size = 2
    hidden_size = 512

    hidden_states = torch.randn(
        seq_len_per_rank, batch_size, hidden_size,
        dtype=torch.bfloat16, device=device,
    )

    print(f"Rank {rank}: Input shape: {hidden_states.shape}")

    # å‰å‘ä¼ æ’­ - CP é€šä¿¡è‡ªåŠ¨å¤„ç†
    with torch.no_grad():
        output, context = transformer_block(
            hidden_states=hidden_states,
            attention_mask=None,
        )

    print(f"Rank {rank}: Output shape: {output.shape}")
    print(f"Rank {rank}: Demo completed!")


if __name__ == "__main__":
    demo_cp_attention()
```

### 8.4 éªŒè¯ CP é…ç½®

```python
# check_cp_config.py
from megatron.core import parallel_state

def print_cp_config():
    """æ‰“å° CP é…ç½®"""
    tp = parallel_state.get_tensor_model_parallel_world_size()
    pp = parallel_state.get_pipeline_model_parallel_world_size()
    cp = parallel_state.get_context_parallel_world_size()
    dp = parallel_state.get_data_parallel_world_size()

    tp_rank = parallel_state.get_tensor_model_parallel_rank()
    pp_rank = parallel_state.get_pipeline_model_parallel_rank()
    cp_rank = parallel_state.get_context_parallel_rank()

    print("=" * 60)
    print("Megatron Parallel Configuration")
    print("=" * 60)
    print(f"TP: {tp} (rank: {tp_rank})")
    print(f"PP: {pp} (rank: {pp_rank})")
    print(f"CP: {cp} (rank: {cp_rank})")
    print(f"DP: {dp}")
    print("=" * 60)
    print(f"Total GPUs: {tp * pp * cp * dp}")

    if cp > 1:
        cp_ranks = parallel_state.get_context_parallel_global_ranks()
        print(f"CP group ranks: {cp_ranks}")

if __name__ == "__main__":
    from megatron import initialize_megatron
    initialize_megatron()
    print_cp_config()
```

### 8.5 å¸¸è§é—®é¢˜è§£å†³

#### NCCL è¶…æ—¶
```bash
export NCCL_BLOCKING_WAIT=1
export NCCL_TIMEOUT=3600
```

#### å†…å­˜ä¸è¶³
```bash
--micro-batch-size 1
--recompute-activations
```

#### åºåˆ—é•¿åº¦ä¸åŒ¹é…
```python
assert seq_length % (2 * context_parallel_size) == 0
```

---

## 9. å¸¸è§é—®é¢˜

### Q1: CP å’Œ Sequence Parallel (SP) çš„åŒºåˆ«ï¼Ÿ

**A:**
- **SP**: åˆ†å‰²éšè—ç»´åº¦çš„ AllReduce/Ring-Reduceï¼Œé€‚ç”¨äº TP
- **CP**: åˆ†å‰²åºåˆ—ç»´åº¦ï¼Œé€‚ç”¨äºè¶…é•¿åºåˆ—

### Q2: ä»€ä¹ˆæ—¶å€™ä½¿ç”¨ CPï¼Ÿ

**A:** å½“åºåˆ—é•¿åº¦è¶…è¿‡å•ä¸ª GPU å†…å­˜å®¹é‡æ—¶ï¼š
- seq_length > 8192 (æ ‡å‡†)
- seq_length > 32768 (å¤§æ¨¡å‹)

### Q3: å¦‚ä½•é€‰æ‹© cp_comm_typeï¼Ÿ

**A:**
- `p2p`: é•¿åºåˆ—ã€ä½å»¶è¿Ÿåœºæ™¯
- `a2a`: Mambaã€SSM æ¨¡å‹
- `allgather`: çŸ­åºåˆ—ã€ç®€å•åœºæ™¯
- `a2a+p2p`: å¤§è§„æ¨¡ CP (>8 ranks)

### Q4: CP æ”¯æŒ PP å—ï¼Ÿ

**A:** æ”¯æŒçš„ï¼Œä½†éœ€è¦è°¨æ…é…ç½®ï¼š
```bash
# CP Ã— PP ç»„åˆ
--context-parallel-size 2 \
--pipeline-model-parallel-size 4
```

### Q5: å¦‚ä½•è°ƒè¯• CP é€šä¿¡ï¼Ÿ

**A:** ä½¿ç”¨ NCCL è°ƒè¯•ç¯å¢ƒå˜é‡ï¼š
```bash
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=COLL

# æŸ¥çœ‹è¿›ç¨‹ç»„
python -c "from megatron.core import parallel_state; \
           print(parallel_state.get_context_parallel_global_ranks())"
```

---

## 10. æ€»ç»“

Context Parallelism æ˜¯ Megatron-LM ä¸­å¤„ç†è¶…é•¿åºåˆ—çš„å…³é”®æŠ€æœ¯ï¼š

1. **æ ¸å¿ƒåŸç†**: Ring Attention + All-to-All é€šä¿¡
2. **æ”¯æŒé€šä¿¡ç±»å‹**: p2p, a2a, allgather, a2a+p2p
3. **é€‚ç”¨åœºæ™¯**: seq_length > 8192 çš„é•¿åºåˆ—è®­ç»ƒ
4. **æ€§èƒ½æƒè¡¡**: CP å¢åŠ é€šä¿¡å¼€é”€ï¼Œä½†çªç ´å• GPU å†…å­˜é™åˆ¶

**æ¨èé…ç½®ï¼š**
```
å°æ¨¡å‹ (<1B): TP=4, PP=1, CP=1
ä¸­æ¨¡å‹ (1B-20B): TP=4, PP=2, CP=1
å¤§æ¨¡å‹ (>20B): TP=4, PP=1, CP=2 (é•¿åºåˆ—)
è¶…å¤§æ¨¡å‹ (>100B): TP=8, PP=4, CP=2
```

---

## 11. å‚è€ƒèµ„æ–™

1. Ring Attention Paper: https://arxiv.org/abs/2310.01889
2. Megatron-LM GitHub: https://github.com/NVIDIA/Megatron-LM
3. Transformer Engine: https://github.com/NVIDIA/TransformerEngine
