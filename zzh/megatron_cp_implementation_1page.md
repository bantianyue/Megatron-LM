# Megatron-LM Context Parallelism (CP) å®ç°åŸç†

> åŸºäº Transformer Engine Ring Attention çš„è¶…é•¿åºåˆ—å¹¶è¡Œæ–¹æ¡ˆ

---

## ğŸ¯ æ ¸å¿ƒåŸç†

**é—®é¢˜**ï¼šå• GPU æ— æ³•å­˜å‚¨è¶…é•¿åºåˆ—çš„ Attention çŸ©é˜µ
**è§£å†³**ï¼šå°†åºåˆ—ç»´åº¦åˆ†å‰²åˆ°å¤šä¸ª GPUï¼Œé€šè¿‡ Ring Attention å®ç°åˆ†æ®µè®¡ç®—

---

## ğŸ“‹ å®ç°æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è¾“å…¥åºåˆ— [0, 1, 2, ..., 8191] (seq_len=8192)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ åˆ†å‰² (CP=2)
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CP Rank 0          â”‚  â”‚  CP Rank 1          â”‚
â”‚  [0-4095]           â”‚  â”‚  [4096-8191]        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â–¼ Ring Attention (P2P)
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Round 0:           â”‚
         â”‚  Rank 0: æœ¬åœ° Attn â”‚
         â”‚  Rank 1: æœ¬åœ° Attn â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚  Round 1:           â”‚
         â”‚  Rank 0 â†” Rank 1   â”‚
         â”‚  äº¤æ¢ KV Cache      â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚  è¾“å‡º AllGather     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å®Œæ•´è¾“å‡º [0, 1, 2, ..., 8191]                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ å…³é”®æŠ€æœ¯ç‚¹

| æŠ€æœ¯ç‚¹ | å®ç°ä½ç½® | è¯´æ˜ |
|--------|---------|------|
| **è¿›ç¨‹ç»„ç®¡ç†** | `parallel_state.py:972` | åˆ›å»º `_CONTEXT_PARALLEL_GROUP` |
| **Ring Attention** | Transformer Engine å†…éƒ¨ | P2P ç¯å½¢é€šä¿¡ |
| **CP Stream** | `transformer_engine.py:1232` | ä¸“ç”¨ CUDA Streamï¼Œé€šä¿¡è®¡ç®—é‡å  |
| **åŠ¨æ€ CP ç»„** | `transformer_engine.py:1346` | è¿è¡Œæ—¶åˆ‡æ¢ CP é…ç½® |

---

## ğŸ’» æ ¸å¿ƒä»£ç 

### 1. é…ç½® CP å‚æ•°

```python
config = TransformerConfig(
    context_parallel_size=2,    # CP = 2
    cp_comm_type="p2p",         # Ring Attention
    sequence_parallel=True,
)
```

### 2. åˆ›å»º Attention å±‚

```python
from megatron.core.extensions.transformer_engine import TEDotProductAttention

attention = TEDotProductAttention(
    config=config,
    layer_number=1,
    attn_mask_type=AttnMaskType.causal,
)
```

### 3. å‰å‘ä¼ æ’­ï¼ˆCP è‡ªåŠ¨å¤„ç†ï¼‰

```python
context, _ = attention(
    query=query,     # [seq/2, batch, heads, dim]
    key=key,
    value=value,
)
# è¾“å‡º: [seq/2, batch, heads, dim]
```

---

## ğŸš€ å¿«é€Ÿå¯åŠ¨

```bash
# ç¯å¢ƒè®¾ç½®
export CUDA_DEVICE_MAX_CONNECTIONS=1

# å¯åŠ¨è®­ç»ƒ (TP=4, CP=2, å…± 8 GPUs)
torchrun --nproc_per_node=8 pretrain_gpt.py \
    --tensor-model-parallel-size 4 \
    --context-parallel-size 2 \
    --cp-comm-type p2p \
    --sequence-parallel \
    --seq-length 8192 \
    --micro-batch-size 1 \
    --global-batch-size 64 \
    --bf16
```

---

## âš¡ æ ¸å¿ƒä¼˜åŠ¿

- **çªç ´å†…å­˜é™åˆ¶**ï¼šæ”¯æŒ 8K-128K è¶…é•¿åºåˆ—è®­ç»ƒ
- **é€šä¿¡ä¼˜åŒ–**ï¼šRing Attention å‡è¡¡é€šä¿¡å¼€é”€
- **é€æ˜é›†æˆ**ï¼šç”¨æˆ·æ— éœ€ä¿®æ”¹æ¨¡å‹ä»£ç 
- **çµæ´»é…ç½®**ï¼šæ”¯æŒ p2p/a2a/a2a+p2p å¤šç§é€šä¿¡æ¨¡å¼

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| é…ç½® | ååé‡ | æœ€å¤§åºåˆ—é•¿åº¦ |
|------|--------|------------|
| TP=8, CP=1 | 180 TFLOPS | 2048 |
| TP=4, CP=2 | 165 TFLOPS (-8%) | 4096 (2x) |
| TP=2, CP=4 | 140 TFLOPS (-22%) | 8192 (4x) |
