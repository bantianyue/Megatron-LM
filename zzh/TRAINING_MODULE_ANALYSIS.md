# Megatron Training æ¨¡å—å®Œæ•´åˆ†æ

## æ¦‚è¿°

`megatron/training/` æ˜¯ Megatron-LM çš„**è®­ç»ƒæ¡†æ¶å±‚**ï¼Œä½äº MCore (megatron/core/) ä¹‹ä¸Šï¼Œæä¾›ç«¯åˆ°ç«¯çš„è®­ç»ƒè„šæœ¬å’Œå·¥å…·ã€‚

**å®šä½**ï¼š
- **é¢å‘å¯¹è±¡**ï¼šæœ€ç»ˆç”¨æˆ·ï¼ˆç ”ç©¶äººå‘˜ã€å·¥ç¨‹å¸ˆï¼‰
- **åŠŸèƒ½**ï¼šè®­ç»ƒæµç¨‹ç®¡ç†ã€å‚æ•°é…ç½®ã€æ—¥å¿—ã€æ£€æŸ¥ç‚¹ç­‰
- **ä¾èµ–**ï¼šMCore ç»„ä»¶

---

## ç›®å½•ç»“æ„

```
megatron/training/
â”‚
â”œâ”€â”€ datasets/                    ğŸ“¦ æ•°æ®é›†å¤„ç†
â”‚   â”œâ”€â”€ data_samplers.py         â†’ æ•°æ®é‡‡æ ·å™¨
â”‚   â”œâ”€â”€ fim_dataset.py           â†’ FIM (Fill-In-Middle) æ•°æ®é›†
â”‚   â”œâ”€â”€ sft_dataset.py           â†’ SFT (Supervised Fine-Tuning) æ•°æ®é›†
â”‚   â””â”€â”€ README.md                â†’ è¯´æ˜æ–‡æ¡£
â”‚
â”œâ”€â”€ tokenizer/                   ğŸ”¤ åˆ†è¯å™¨
â”‚   â”œâ”€â”€ tokenizer.py             â†’ åˆ†è¯å™¨åŸºç±»
â”‚   â”œâ”€â”€ bert_tokenization.py     â†’ BERT åˆ†è¯å™¨
â”‚   â”œâ”€â”€ gpt2_tokenization.py     â†’ GPT-2 åˆ†è¯å™¨
â”‚   â”œâ”€â”€ multimodal_tokenizer.py  â†’ å¤šæ¨¡æ€åˆ†è¯å™¨
â”‚   â””â”€â”€ sft_tokenizer.py         â†’ SFT åˆ†è¯å™¨
â”‚
â””â”€â”€ [æ ¸å¿ƒè®­ç»ƒæ¨¡å—]               ğŸ¯ è§ä¸‹æ–¹è¯¦è§£
```

---

## æ ¸å¿ƒè®­ç»ƒæ¨¡å—è¯¦è§£

### 1ï¸âƒ£ **è®­ç»ƒå…¥å£ & æµç¨‹**

#### `training.py` - ä¸»è®­ç»ƒå¾ªç¯
**åŠŸèƒ½**ï¼šå®Œæ•´çš„è®­ç»ƒæµç¨‹å®ç°

**æ ¸å¿ƒå‡½æ•°**ï¼š
```python
def pretrain(..., train_valid_test_dataset_provider, model_provider, ...)
```

**ä¸»è¦åŠŸèƒ½**ï¼š
- è®­ç»ƒå¾ªç¯ä¸»é€»è¾‘
- å‰å‘/åå‘ä¼ æ’­åè°ƒ
- ä¼˜åŒ–å™¨æ­¥éª¤
- æ£€æŸ¥ç‚¹ä¿å­˜
- éªŒè¯å’Œè¯„ä¼°

**ä»£ç ç»“æ„**ï¼š
```python
def pretrain(...):
    # 1. åˆå§‹åŒ–
    initialize_megatron(...)

    # 2. æ„å»ºæ¨¡å‹
    model = model_provider(...)

    # 3. æ„å»ºæ•°æ®åŠ è½½å™¨
    dataloader = build_pretraining_data_loader(...)

    # 4. è·å–å‰å‘/åå‘å‡½æ•°ï¼ˆæ”¯æŒæµæ°´çº¿å¹¶è¡Œï¼‰
    forward_backward_func = get_forward_backward_func(...)

    # 5. è®­ç»ƒå¾ªç¯
    for iteration in range(...):
        # å‰å‘/åå‘
        forward_backward_func(...)

        # æ¢¯åº¦åŒæ­¥
        finalize_model_grads(...)

        # ä¼˜åŒ–å™¨æ­¥éª¤
        optimizer.step()

        # æ£€æŸ¥ç‚¹
        if iteration % checkpoint_interval == 0:
            save_checkpoint(...)
```

**å‚è€ƒ**ï¼š`megatron/training/training.py`

---

#### `initialize.py` - åˆå§‹åŒ–
**åŠŸèƒ½**ï¼šåˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒå’Œç»„ä»¶åˆå§‹åŒ–

**ä¸»è¦å‡½æ•°**ï¼š
```python
def initialize_megatron(
    extra_args_provider=None,
    ignore_unknown_args=False,
    allow_no_cuda=False,
    skip_mcore_initialization=False,
):
```

**åˆå§‹åŒ–æµç¨‹**ï¼š
1. è§£æå‘½ä»¤è¡Œå‚æ•°
2. è®¾ç½®éšæœºç§å­
3. åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
4. åˆå§‹åŒ–æ¨¡å‹å¹¶è¡Œï¼ˆTP/PP/DP/CPï¼‰
5. åˆå§‹åŒ–å…¨å±€å˜é‡
6. è®¾ç½®æ—¥å¿—
7. åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœæ¢å¤è®­ç»ƒï¼‰

**å…³é”®æ“ä½œ**ï¼š
- è¿›ç¨‹ç»„åˆ›å»ºï¼ˆ`dist.init_process_group`ï¼‰
- CUDA è®¾å¤‡è®¾ç½®
- å†…å­˜ç¼“å†²åŒºåˆ†é…
- å¹¶è¡ŒçŠ¶æ€åˆå§‹åŒ–ï¼ˆ`parallel_state.initialize_model_parallel`ï¼‰

**å‚è€ƒ**ï¼š`megatron/training/initialize.py`

---

### 2ï¸âƒ£ **å‚æ•°é…ç½®ç³»ç»Ÿ**

#### `arguments.py` - å‘½ä»¤è¡Œå‚æ•°
**åŠŸèƒ½**ï¼šå®šä¹‰å’Œç®¡ç†æ‰€æœ‰è®­ç»ƒå‚æ•°

**ä¸»è¦å‡½æ•°**ï¼š
```python
def add_megatron_arguments(parser):
    """æ·»åŠ  Megatron ç‰¹å®šå‚æ•°"""

def parse_args(extra_args_provider=None, ignore_unknown_args=False):
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
```

**å‚æ•°ç±»åˆ«**ï¼š
- **æ¨¡å‹å‚æ•°**ï¼š`--hidden-size`, `--num-layers`, `--num-attention-heads`
- **å¹¶è¡Œå‚æ•°**ï¼š`--tensor-model-parallel-size`, `--pipeline-model-parallel-size`
- **è®­ç»ƒå‚æ•°**ï¼š`--batch-size`, `--lr`, `--seq-length`
- **ä¼˜åŒ–å‚æ•°**ï¼š`--optimizer`, `--weight-decay`
- **ç²¾åº¦å‚æ•°**ï¼š`--fp16`, `--bf16`
- **æ£€æŸ¥ç‚¹å‚æ•°**ï¼š`--save`, `--load`
- **æ—¥å¿—å‚æ•°**ï¼š`--log-interval`, `--tensorboard-queue-size`

**ç¤ºä¾‹**ï¼š
```bash
python pretrain_gpt.py \
    --tensor-model-parallel-size 8 \
    --pipeline-model-parallel-size 4 \
    --num-layers 96 \
    --hidden-size 12288 \
    --num-attention-heads 96 \
    --batch-size 8 \
    --lr 1e-4
```

**å‚è€ƒ**ï¼š`megatron/training/arguments.py`

---

#### `argument_utils.py` - å‚æ•°å·¥å…·
**åŠŸèƒ½**ï¼šå‚æ•°å¤„ç†å’ŒéªŒè¯å·¥å…·

**ä¸»è¦åŠŸèƒ½**ï¼š
- å‚æ•°ç±»å‹è½¬æ¢
- å‚æ•°éªŒè¯
- é»˜è®¤å€¼å¤„ç†

---

#### `yaml_arguments.py` - YAML é…ç½®
**åŠŸèƒ½**ï¼šæ”¯æŒä» YAML æ–‡ä»¶åŠ è½½é…ç½®

**ç”¨é€”**ï¼šé¿å…è¶…é•¿å‘½ä»¤è¡Œï¼Œæ”¯æŒé…ç½®æ–‡ä»¶

**ç¤ºä¾‹**ï¼š
```yaml
# config.yaml
model:
  hidden_size: 12288
  num_layers: 96
  num_attention_heads: 96

training:
  batch_size: 8
  lr: 1e-4
  seq_length: 4096
```

---

#### `training_config.py` - è®­ç»ƒé…ç½®
**åŠŸèƒ½**ï¼šè®­ç»ƒç›¸å…³çš„é…ç½®ç±»

---

#### `common_config.py` - é€šç”¨é…ç½®
**åŠŸèƒ½**ï¼šè·¨æ¨¡å—å…±äº«çš„é…ç½®

---

### 3ï¸âƒ£ **æ£€æŸ¥ç‚¹ç®¡ç†**

#### `checkpointing.py` - æ£€æŸ¥ç‚¹ä¿å­˜/åŠ è½½
**åŠŸèƒ½**ï¼šæ¨¡å‹çŠ¶æ€ä¿å­˜å’Œæ¢å¤

**ä¸»è¦å‡½æ•°**ï¼š
```python
def save_checkpoint(iteration, model, optimizer, opt_param_scheduler, ...)
def load_checkpoint(model, optimizer, opt_param_scheduler, ...)
def checkpoint_exists(iteration)
```

**ä¿å­˜å†…å®¹**ï¼š
- æ¨¡å‹å‚æ•°
- ä¼˜åŒ–å™¨çŠ¶æ€
- å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
- éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€
- è®­ç»ƒè¿­ä»£æ¬¡æ•°

**åˆ†å¸ƒå¼æ£€æŸ¥ç‚¹**ï¼š
- ä¸ `megatron/core/dist_checkpointing/` é…åˆ
- åˆ†ç‰‡ä¿å­˜/åŠ è½½
- æ”¯æŒ FSDPã€æ¨¡å‹å¹¶è¡Œ

**å‚è€ƒ**ï¼š`megatron/training/checkpointing.py`

---

### 4ï¸âƒ£ **å…¨å±€çŠ¶æ€ç®¡ç†**

#### `global_vars.py` - å…¨å±€å˜é‡
**åŠŸèƒ½**ï¼šç®¡ç†è®­ç»ƒè¿‡ç¨‹ä¸­çš„å…¨å±€çŠ¶æ€

**å…¨å±€å˜é‡**ï¼š
```python
# å…¨å±€å˜é‡ï¼ˆè¿›ç¨‹çº§ï¼‰
_args = None                           # å‘½ä»¤è¡Œå‚æ•°
_signal_handler = None                 # ä¿¡å·å¤„ç†å™¨
_tokenizer = None                      # åˆ†è¯å™¨
_tensorboard_writer = None            # TensorBoard å†™å…¥å™¨
_wandb_writer = None                  # WandB å†™å…¥å™¨
_one_logger = None                     # OneDocker æ—¥å¿—å™¨
_adlr_autoresume = None               # ADLR è‡ªåŠ¨æ¢å¤
_timers = None                         # æ€§èƒ½è®¡æ—¶å™¨
_num_microbatches_calculator = None   # å¾®æ‰¹æ¬¡è®¡ç®—å™¨
_memory_buffer = None                 # å†…å­˜ç¼“å†²åŒº
```

**è®¿é—®å‡½æ•°**ï¼š
```python
get_args()
get_tokenizer()
get_timers()
get_tensorboard_writer()
get_wandb_writer()
get_one_logger()
```

**ç”¨é€”**ï¼š
- é¿å…å…¨å±€ä¼ é€’å‚æ•°
- æä¾›ç»Ÿä¸€çš„è®¿é—®æ¥å£
- ç®€åŒ–ä»£ç 

**å‚è€ƒ**ï¼š`megatron/training/global_vars.py`

---

### 5ï¸âƒ£ **å·¥å…·å‡½æ•°**

#### `utils.py` - é€šç”¨å·¥å…·
**åŠŸèƒ½**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­çš„å¸¸ç”¨å·¥å…·å‡½æ•°

**ä¸»è¦å‡½æ•°**ï¼š
```python
def print_rank_0(message)          # åœ¨ rank 0 æ‰“å°
def is_last_rank()                 # åˆ¤æ–­æ˜¯å¦æœ€åä¸€ä¸ª rank
def print_rank_last(message)       # åœ¨æœ€åä¸€ä¸ª rank æ‰“å°
def report_memory()                # æŠ¥å‘Šå†…å­˜ä½¿ç”¨
def calc_params_l2_norm(model)    # è®¡ç®— L2 èŒƒæ•°
def average_metrics_across_data_parallel_group(metrics)  # è·¨ DP å¹³å‡æŒ‡æ ‡
```

**ç”¨é€”**ï¼š
- æ—¥å¿—è¾“å‡º
- å†…å­˜ç›‘æ§
- æŒ‡æ ‡èšåˆ
- å‚æ•°ç»Ÿè®¡

**å‚è€ƒ**ï¼š`megatron/training/utils.py`

---

#### `async_utils.py` - å¼‚æ­¥å·¥å…·
**åŠŸèƒ½**ï¼šå¼‚æ­¥ä¿å­˜æ£€æŸ¥ç‚¹

**ä¸»è¦åŠŸèƒ½**ï¼š
- åå°ä¿å­˜æ£€æŸ¥ç‚¹
- ä¸é˜»å¡è®­ç»ƒ
- æå‡è®­ç»ƒæ•ˆç‡

**ä¸»è¦å‡½æ•°**ï¼š
```python
def save_checkpoint(async_save, ...)
def finalize_async_save(async_save)
```

**å‚è€ƒ**ï¼š`megatron/training/async_utils.py`

---

#### `theoretical_memory_usage.py` - å†…å­˜åˆ†æ
**åŠŸèƒ½**ï¼šè®¡ç®—ç†è®ºå†…å­˜ä½¿ç”¨

**ä¸»è¦å‡½æ•°**ï¼š
```python
def report_theoretical_memory(config, model_type, dp_world_size, vp_size)
```

**åˆ†æå†…å®¹**ï¼š
- æ¨¡å‹å‚æ•°å†…å­˜
- æ¢¯åº¦å†…å­˜
- ä¼˜åŒ–å™¨çŠ¶æ€å†…å­˜
- æ¿€æ´»å†…å­˜
- æ€»å†…å­˜éœ€æ±‚

**ç”¨é€”**ï¼š
- è®­ç»ƒå‰é¢„ä¼°å†…å­˜éœ€æ±‚
- é€‰æ‹©åˆé€‚çš„å¹¶è¡Œç­–ç•¥
- ä¼˜åŒ–æ‰¹æ¬¡å¤§å°

**å‚è€ƒ**ï¼š`megatron/training/theoretical_memory_usage.py`

---

### 6ï¸âƒ£ **æ—¥å¿— & ç›‘æ§**

#### `log_handler.py` - æ—¥å¿—å¤„ç†å™¨
**åŠŸèƒ½**ï¼šè‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨

**ç‰¹æ€§**ï¼š
- è¿‡æ»¤é Megatron æ—¥å¿—
- å½©è‰²è¾“å‡º
- ç­‰çº§æ§åˆ¶

**å‚è€ƒ**ï¼š`megatron/training/log_handler.py`

---

#### `wandb_utils.py` - WandB é›†æˆ
**åŠŸèƒ½**ï¼šWeights & Biases å®éªŒè·Ÿè¸ª

**ä¸»è¦åŠŸèƒ½**ï¼š
- åˆå§‹åŒ– WandB
- è®°å½•æŒ‡æ ‡
- å¯è§†åŒ–è®­ç»ƒæ›²çº¿

**å‚è€ƒ**ï¼š`megatron/training/wandb_utils.py`

---

#### `one_logger_utils.py` - OneLogger é›†æˆ
**åŠŸèƒ½**ï¼šNVIDIA OneDocker æ—¥å¿—ç³»ç»Ÿ

**å‚è€ƒ**ï¼š`megatron/training/one_logger_utils.py`

---

#### `dist_signal_handler.py` - åˆ†å¸ƒå¼ä¿¡å·å¤„ç†
**åŠŸèƒ½**ï¼šå¤„ç†åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„ä¿¡å·

**ä¸»è¦åŠŸèƒ½**ï¼š
- ä¼˜é›…é€€å‡º
- æ£€æŸ¥ç‚¹ä¿å­˜
- ä¿¡å·åŒæ­¥

**å‚è€ƒ**ï¼š`megatron/training/dist_signal_handler.py`

---

### 7ï¸âƒ£ **é«˜çº§ç‰¹æ€§**

#### `ft_integration.py` - Fault Tolerance é›†æˆ
**åŠŸèƒ½**ï¼šå®¹é”™è®­ç»ƒæ”¯æŒ

**ç‰¹æ€§**ï¼š
- è‡ªåŠ¨æ•…éšœæ¢å¤
- æ£€æŸ¥ç‚¹å›æ»š
- å¼¹æ€§è®­ç»ƒ

**å‚è€ƒ**ï¼š`megatron/training/ft_integration.py`

---

#### `resilience_config.py` - å¼¹æ€§é…ç½®
**åŠŸèƒ½**ï¼šå®¹é”™å’Œå¼¹æ€§ç›¸å…³é…ç½®

**å‚è€ƒ**ï¼š`megatron/training/resilience_config.py`

---

#### `inprocess_restart.py` - è¿›ç¨‹å†…é‡å¯
**åŠŸèƒ½**ï¼šæ”¯æŒè¿›ç¨‹å†…é‡å¯è®­ç»ƒ

**ä¸»è¦åŠŸèƒ½**ï¼š
- æ— éœ€é‡å¯è¿›ç¨‹
- é‡æ–°åˆå§‹åŒ–æ¨¡å‹
- çŠ¶æ€æ¢å¤

**å‚è€ƒ**ï¼š`megatron/training/inprocess_restart.py`

---

## å­æ¨¡å—è¯¦è§£

### ğŸ“¦ datasets/ - æ•°æ®é›†å¤„ç†

#### `data_samplers.py` - æ•°æ®é‡‡æ ·å™¨
**åŠŸèƒ½**ï¼šåˆ†å¸ƒå¼æ•°æ®é‡‡æ ·

**ä¸»è¦ç±»**ï¼š
```python
class MegatronPretrainingSampler:
    """é¢„è®­ç»ƒæ•°æ®é‡‡æ ·å™¨"""
    - æ”¯æŒ dp_rank é‡‡æ ·
    - æ”¯æŒéšæœºç§å­
    - æ”¯æŒå¤šè½®è®­ç»ƒ

class MegatronPretrainingRandomSampler:
    """éšæœºé¢„è®­ç»ƒé‡‡æ ·å™¨"""
```

**ç‰¹æ€§**ï¼š
- åˆ†å¸ƒå¼é‡‡æ ·
- æ— é‡å¤é‡‡æ ·
- ç§å­å¯å¤ç°

**å‚è€ƒ**ï¼š`megatron/training/datasets/data_samplers.py`

---

#### `fim_dataset.py` - FIM æ•°æ®é›†
**åŠŸèƒ½**ï¼šFill-In-Middle æ ¼å¼æ•°æ®é›†

**ç”¨é€”**ï¼š
- ä»£ç è¡¥å…¨
- æ–‡æœ¬è¡¥å…¨
- ä¸­é—´å¡«å……ä»»åŠ¡

**å‚è€ƒ**ï¼š`megatron/training/datasets/fim_dataset.py`

---

#### `sft_dataset.py` - SFT æ•°æ®é›†
**åŠŸèƒ½**ï¼šç›‘ç£å¾®è°ƒæ•°æ®é›†

**ç”¨é€”**ï¼š
- æŒ‡ä»¤å¾®è°ƒ
- å¯¹è¯æ•°æ®
- é—®ç­”æ•°æ®

**å‚è€ƒ**ï¼š`megatron/training/datasets/sft_dataset.py`

---

### ğŸ”¤ tokenizer/ - åˆ†è¯å™¨

#### `tokenizer.py` - åˆ†è¯å™¨åŸºç±»
**åŠŸèƒ½**ï¼šæŠ½è±¡åˆ†è¯å™¨æ¥å£

**ä¸»è¦ç±»**ï¼š
```python
class AbstractTokenizer:
    def tokenize(self, text):
        """åˆ†è¯"""
    def detokenize(self, tokens):
        """ååˆ†è¯"""
    @property
    def vocab_size(self):
        """è¯æ±‡è¡¨å¤§å°"""
```

**å‚è€ƒ**ï¼š`megatron/training/tokenizer/tokenizer.py`

---

#### `bert_tokenization.py` - BERT åˆ†è¯å™¨
**åŠŸèƒ½**ï¼šBERT WordPiece åˆ†è¯

**ç‰¹æ€§**ï¼š
- WordPiece åˆ†è¯
- æ”¯æŒå¤šè¯­è¨€

**å‚è€ƒ**ï¼š`megatron/training/tokenizer/bert_tokenization.py`

---

#### `gpt2_tokenization.py` - GPT-2 åˆ†è¯å™¨
**åŠŸèƒ½**ï¼šGPT-2 BPE åˆ†è¯

**ç‰¹æ€§**ï¼š
- BPE åˆ†è¯
- å­—èŠ‚çº§ç¼–ç 

**å‚è€ƒ**ï¼š`megatron/training/tokenizer/gpt2_tokenization.py`

---

#### `multimodal_tokenizer.py` - å¤šæ¨¡æ€åˆ†è¯å™¨
**åŠŸèƒ½**ï¼šå¤šæ¨¡æ€æ•°æ®åˆ†è¯

**æ”¯æŒ**ï¼š
- æ–‡æœ¬ + å›¾åƒ
- æ–‡æœ¬ + è§†é¢‘
- å¤šæ¨¡æ€å¯¹é½

**å‚è€ƒ**ï¼š`megatron/training/tokenizer/multimodal_tokenizer.py`

---

#### `sft_tokenizer.py` - SFT åˆ†è¯å™¨
**åŠŸèƒ½**ï¼šç›‘ç£å¾®è°ƒä¸“ç”¨åˆ†è¯å™¨

**ç‰¹æ€§**ï¼š
- å¯¹è¯æ ¼å¼
- ç‰¹æ®Šæ ‡è®°å¤„ç†

**å‚è€ƒ**ï¼š`megatron/training/tokenizer/sft_tokenizer.py`

---

## æ¨¡å—é—´å…³ç³»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    è®­ç»ƒå…¥å£                             â”‚
â”‚  [training.py] pretrain()                              â”‚
â”‚  [initialize.py] initialize_megatron()                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼           â–¼           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚å‚æ•°é…ç½®  â”‚ â”‚å…¨å±€çŠ¶æ€  â”‚ â”‚ å·¥å…·å‡½æ•° â”‚
    â”‚argumentsâ”‚ â”‚global_varsâ”‚  â”‚ utils   â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚           â”‚
         â–¼           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      æ•°æ®ç®¡é“            â”‚
    â”‚  [datasets/] + [tokenizer/]â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      MCore ç»„ä»¶          â”‚
    â”‚  megatron/core/          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ä½¿ç”¨æµç¨‹

### å…¸å‹è®­ç»ƒæµç¨‹

```python
# 1. å¯¼å…¥è®­ç»ƒæ¨¡å—
from megatron.training import initialize_megatron, pretrain
from megatron.training import get_args, get_tokenizer
from megatron.core import mpu

# 2. åˆå§‹åŒ–
initialize_megatron(extra_args_provider=None)

# 3. è·å–å‚æ•°
args = get_args()
tokenizer = get_tokenizer()

# 4. å®šä¹‰æ¨¡å‹æä¾›è€…
def model_provider():
    """æ„å»ºæ¨¡å‹"""
    from megatron.core.models.gpt import GPTModel
    return GPTModel(config=num_layers, ...)

# 5. å®šä¹‰æ•°æ®é›†æä¾›è€…
def dataset_provider():
    """æ„å»ºæ•°æ®é›†"""
    from megatron.training.datasets import build_pretraining_data_loader
    return build_pretraining_data_loader(...)

# 6. å¼€å§‹è®­ç»ƒ
pretrain(
    train_valid_test_dataset_provider=dataset_provider,
    model_provider=model_provider,
)
```

---

## ä¸ MCore çš„å…³ç³»

| Training Framework | MCore (megatron/core/) |
|-------------------|------------------------|
| **é«˜å±‚ API** | **åº•å±‚å®ç°** |
| `training.py` - è®­ç»ƒå¾ªç¯ | `transformer/` - Transformer å±‚ |
| `arguments.py` - å‚æ•°ç®¡ç† | `models/` - æ¨¡å‹å®šä¹‰ |
| `checkpointing.py` - æ£€æŸ¥ç‚¹ç®¡ç† | `dist_checkpointing/` - åˆ†å¸ƒå¼æ£€æŸ¥ç‚¹ |
| `initialize.py` - åˆå§‹åŒ– | `parallel_state.py` - å¹¶è¡ŒçŠ¶æ€ |
| `datasets/` - æ•°æ®åŠ è½½ | `datasets/` - æ•°æ®é›†å®ç° |
| `tokenizer/` - åˆ†è¯å™¨ | - |
| `utils.py` - å·¥å…·å‡½æ•° | `utils/` - åº•å±‚å·¥å…· |

---

## å…³é”®æ–‡ä»¶ç´¢å¼•

| æ–‡ä»¶ | è¡Œæ•°ä¼°è®¡ | åŠŸèƒ½ |
|------|---------|------|
| `training.py` | 2000+ | ä¸»è®­ç»ƒå¾ªç¯ |
| `initialize.py` | 500+ | åˆå§‹åŒ–æµç¨‹ |
| `arguments.py` | 800+ | å‚æ•°å®šä¹‰ |
| `checkpointing.py` | 1000+ | æ£€æŸ¥ç‚¹ç®¡ç† |
| `global_vars.py` | 200+ | å…¨å±€å˜é‡ |
| `utils.py` | 300+ | å·¥å…·å‡½æ•° |
| `datasets/data_samplers.py` | 400+ | æ•°æ®é‡‡æ · |
| `tokenizer/tokenizer.py` | 300+ | åˆ†è¯å™¨åŸºç±» |

---

## æ€»ç»“

### Training Framework çš„æ ¸å¿ƒèŒè´£

1. **è®­ç»ƒæµç¨‹ç®¡ç†**ï¼š`training.py`, `initialize.py`
2. **å‚æ•°é…ç½®**ï¼š`arguments.py`, `yaml_arguments.py`
3. **çŠ¶æ€ç®¡ç†**ï¼š`global_vars.py`, `checkpointing.py`
4. **æ•°æ®ç®¡é“**ï¼š`datasets/`, `tokenizer/`
5. **å·¥å…·æ”¯æŒ**ï¼š`utils.py`, `async_utils.py`
6. **æ—¥å¿—ç›‘æ§**ï¼š`log_handler.py`, `wandb_utils.py`
7. **é«˜çº§ç‰¹æ€§**ï¼š`ft_integration.py`, `inprocess_restart.py`

### è®¾è®¡ç‰¹ç‚¹

- âœ… **ç”¨æˆ·å‹å¥½**ï¼šæä¾›é«˜å±‚ APIï¼Œéšè—å¤æ‚ç»†èŠ‚
- âœ… **é…ç½®çµæ´»**ï¼šæ”¯æŒå‘½ä»¤è¡Œã€YAMLã€ç¼–ç¨‹å¼é…ç½®
- âœ… **å¯æ‰©å±•**ï¼šæ˜“äºæ·»åŠ æ–°çš„è®­ç»ƒç­–ç•¥
- âœ… **ç”Ÿäº§å°±ç»ª**ï¼šå®Œå–„çš„æ—¥å¿—ã€æ£€æŸ¥ç‚¹ã€å®¹é”™æœºåˆ¶

### ä¸ MCore çš„åˆ†å·¥

**Training Framework (megatron/training/)**ï¼š
- é¢å‘æœ€ç»ˆç”¨æˆ·
- æä¾›ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹
- å¤„ç†å‚æ•°ã€æ—¥å¿—ã€æ£€æŸ¥ç‚¹ç­‰

**MCore (megatron/core/)**ï¼š
- é¢å‘æ¡†æ¶å¼€å‘è€…
- æä¾›å¯é‡ç”¨çš„æ„å»ºå—
- å®ç°å¹¶è¡Œç­–ç•¥ã€ä¼˜åŒ–å™¨ã€æ¨¡å‹ç­‰

---

*åŸºäº Megatron-LM ä»£ç åˆ†æ*
*åˆ†ææ—¥æœŸ: 2025-01-30*
